<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Understanding Accuracy Metrics in EcoExtract • ecoextract</title><script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="Understanding Accuracy Metrics in EcoExtract"></head><body><p>








  pkgdown/mermaid.html

  

  </p>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">ecoextract</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.2</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="index.html" aria-label="Home"><span class="fa fa-home"></span></a></li>
<li class="nav-item"><a class="nav-link" href="articles/getting-started.html">Getting Started</a></li>
<li class="nav-item"><a class="nav-link" href="articles/ecoextract-workflow.html">Workflow</a></li>
<li class="nav-item"><a class="nav-link" href="articles/configuration.html">Configuration</a></li>
<li class="nav-item"><a class="nav-link" href="articles/testing.html">Testing</a></li>
<li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/n8layman/ecoextract/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-title-body">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Understanding Accuracy Metrics in EcoExtract</h1>
      <small class="dont-index">Source: <a href="https://github.com/n8layman/ecoextract/blob/main/ACCURACY.md" class="external-link"><code>ACCURACY.md</code></a></small>
    </div>

<div id="understanding-accuracy-metrics-in-ecoextract" class="section level1">

<div class="section level2">
<h2 id="overview">Overview<a class="anchor" aria-label="anchor" href="#overview"></a></h2>
<p>EcoExtract calculates accuracy by analyzing human edits to model-extracted records. When you review documents using <a href="https://github.com/n8layman/ecoreview" class="external-link">ecoreview</a>, all edits are tracked in an audit table (<code>record_edits</code>). The <code><a href="reference/calculate_accuracy.html">calculate_accuracy()</a></code> function analyzes these edits to provide comprehensive accuracy metrics.</p>
</div>
<div class="section level2">
<h2 id="philosophy-two-separate-questions">Philosophy: Two Separate Questions<a class="anchor" aria-label="anchor" href="#philosophy-two-separate-questions"></a></h2>
<p>Traditional accuracy metrics treat extraction as all-or-nothing: a record is either “correct” or “incorrect.” But in practice, extraction quality has two independent dimensions:</p>
<ol style="list-style-type: decimal"><li>
<strong>Record Detection</strong>: Did the model find the record, or did it miss it? Did it hallucinate records that don’t exist?</li>
<li>
<strong>Field Accuracy</strong>: Of the records the model found, how accurate were the individual fields?</li>
</ol><p>EcoExtract separates these concerns to give you a more nuanced understanding of model performance.</p>
</div>
<div class="section level2">
<h2 id="core-concepts">Core Concepts<a class="anchor" aria-label="anchor" href="#core-concepts"></a></h2>
<div class="section level3">
<h3 id="ground-truth">Ground Truth<a class="anchor" aria-label="anchor" href="#ground-truth"></a></h3>
<p>Ground truth is established through human review: - <strong>Model extractions</strong> (records with <code>added_by_user = 0</code> or NULL) - <strong>Human additions</strong> (records with <code>added_by_user = 1</code>) indicate records the model missed - <strong>Soft deletes</strong> (records with <code>deleted_by_user</code> set) indicate hallucinations - <strong>Field edits</strong> (tracked in <code>record_edits</code> table) indicate which specific fields were wrong</p>
<p>Only documents marked as reviewed (<code>reviewed_at IS NOT NULL</code>) are included in accuracy calculations.</p>
</div>
<div class="section level3">
<h3 id="verified-documents">Verified Documents<a class="anchor" aria-label="anchor" href="#verified-documents"></a></h3>
<p>Accuracy is only calculated for documents that have been human-reviewed. This ensures metrics reflect actual human judgment rather than unverified model output.</p>
</div>
</div>
<div class="section level2">
<h2 id="metrics-explained">Metrics Explained<a class="anchor" aria-label="anchor" href="#metrics-explained"></a></h2>
<div class="section level3">
<h3 id="raw-counts">Raw Counts<a class="anchor" aria-label="anchor" href="#raw-counts"></a></h3>
<p>These are the fundamental measurements from which all other metrics derive:</p>
<ul><li>
<strong>verified_documents</strong>: Number of documents that have been reviewed</li>
<li>
<strong>verified_records</strong>: Total records across all verified documents</li>
<li>
<strong>model_extracted</strong>: Records the model extracted (may include hallucinations)</li>
<li>
<strong>human_added</strong>: Records humans added because model missed them (false negatives)</li>
<li>
<strong>deleted</strong>: Records humans soft-deleted because model hallucinated them (false positives)</li>
<li>
<strong>records_with_edits</strong>: Records that had at least one field corrected</li>
<li>
<strong>column_edits</strong>: Count of edits per column (named vector)</li>
</ul></div>
<div class="section level3">
<h3 id="record-detection-metrics">Record Detection Metrics<a class="anchor" aria-label="anchor" href="#record-detection-metrics"></a></h3>
<p>These answer: “Did the model find the records?”</p>
<div class="section level4">
<h4 id="records-classification">Records Classification<a class="anchor" aria-label="anchor" href="#records-classification"></a></h4>
<ul><li>
<strong>records_found</strong>: <code>model_extracted - deleted</code>
<ul><li>True positives: Records the model found (even if imperfect)</li>
</ul></li>
<li>
<strong>records_missed</strong>: <code>human_added</code>
<ul><li>False negatives: Records that exist but model didn’t find</li>
</ul></li>
<li>
<strong>records_hallucinated</strong>: <code>deleted</code>
<ul><li>False positives: Records model made up that don’t exist</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="detection-performance">Detection Performance<a class="anchor" aria-label="anchor" href="#detection-performance"></a></h4>
<ul><li>
<strong>detection_precision</strong>: <code>records_found / model_extracted</code>
<ul><li>Of everything the model extracted, what fraction was real (not hallucinated)?</li>
<li>High precision means few hallucinations</li>
</ul></li>
<li>
<strong>detection_recall</strong>: <code>records_found / (records_found + records_missed)</code>
<ul><li>Of all true records, what fraction did the model find?</li>
<li>High recall means few missed records</li>
</ul></li>
<li>
<strong>perfect_record_rate</strong>: <code>(records_found - records_with_edits) / records_found</code>
<ul><li>Of records the model found, what fraction had zero errors?</li>
<li>Measures extraction quality for found records</li>
</ul></li>
</ul></div>
</div>
<div class="section level3">
<h3 id="field-level-metrics">Field-Level Metrics<a class="anchor" aria-label="anchor" href="#field-level-metrics"></a></h3>
<p>These answer: “How accurate were the individual fields?”</p>
<p>Field-level metrics provide partial credit. If a record has 10 fields and the model got 9 correct, it’s credited with 9 correct fields rather than being treated as completely wrong.</p>
<div class="section level4">
<h4 id="field-calculations">Field Calculations<a class="anchor" aria-label="anchor" href="#field-calculations"></a></h4>
<ul><li>
<strong>total_fields</strong>: <code>model_extracted × num_fields</code>
<ul><li>Total fields the model attempted to extract</li>
</ul></li>
<li>
<strong>correct_fields</strong>: <code>total_fields - deleted_fields - edited_fields</code>
<ul><li>Fields that were extracted correctly</li>
<li>deleted_fields = <code>deleted × num_fields</code> (all fields in deleted records are wrong)</li>
<li>edited_fields = <code>sum(column_edits)</code> (fields that humans corrected)</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="field-performance">Field Performance<a class="anchor" aria-label="anchor" href="#field-performance"></a></h4>
<ul><li>
<strong>field_precision</strong>: <code>correct_fields / total_fields</code>
<ul><li>Of all fields the model extracted, what fraction was correct?</li>
</ul></li>
<li>
<strong>field_recall</strong>: <code>correct_fields / true_fields</code>
<ul><li>true_fields = <code>(records_found × num_fields) + (human_added × num_fields)</code>
</li>
<li>Of all true fields that should exist, what fraction did we extract correctly?</li>
</ul></li>
<li>
<strong>field_f1</strong>: Harmonic mean of field precision and recall
<ul><li><code>2 × precision × recall / (precision + recall)</code></li>
<li>Balanced measure combining both metrics</li>
</ul></li>
</ul></div>
</div>
<div class="section level3">
<h3 id="per-column-accuracy">Per-Column Accuracy<a class="anchor" aria-label="anchor" href="#per-column-accuracy"></a></h3>
<ul><li>
<strong>column_accuracy</strong>: Named vector showing accuracy for each column
<ul><li>Formula: <code>1 - (column_edits / model_extracted)</code>
</li>
<li>Example: If 100 records extracted and “species” field edited 15 times, species accuracy = 0.85</li>
<li>Shows which fields are hardest for the model to extract</li>
<li>Helps identify where to improve prompts or schemas</li>
</ul></li>
</ul></div>
<div class="section level3">
<h3 id="edit-severity-metrics">Edit Severity Metrics<a class="anchor" aria-label="anchor" href="#edit-severity-metrics"></a></h3>
<p>Not all edits are equal. EcoExtract classifies edits based on the schema:</p>
<div class="section level4">
<h4 id="major-vs-minor-edits">Major vs Minor Edits<a class="anchor" aria-label="anchor" href="#major-vs-minor-edits"></a></h4>
<p><strong>Major edits</strong> affect fields that identify or are required for the record: - <strong>Unique fields</strong> (<code>x-unique-fields</code> in schema): Fields that identify the record - Example: species names, dates, locations that distinguish one record from another - <strong>Required fields</strong> (<code>required</code> in schema): Fields mandatory for record validity - Example: supporting sentences, organisms_identifiable</p>
<p><strong>Minor edits</strong> affect optional descriptive fields: - Fields that provide additional detail but aren’t core to record identity - Example: publication_year, page_number</p>
</div>
<div class="section level4">
<h4 id="severity-metrics">Severity Metrics<a class="anchor" aria-label="anchor" href="#severity-metrics"></a></h4>
<ul><li>
<strong>major_edits</strong>: Count of edits to unique or required fields</li>
<li>
<strong>minor_edits</strong>: Count of edits to other fields</li>
<li>
<strong>major_edit_rate</strong>: <code>major_edits / (major_edits + minor_edits)</code>
<ul><li>Closer to 1.0 means most errors are in critical fields</li>
<li>Closer to 0.0 means most errors are in minor details</li>
</ul></li>
<li>
<strong>avg_edits_per_document</strong>: <code>total_edits / verified_documents</code>
<ul><li>On average, how many field corrections per document?</li>
<li>Lower is better</li>
</ul></li>
</ul></div>
</div>
</div>
<div class="section level2">
<h2 id="example-scenario">Example Scenario<a class="anchor" aria-label="anchor" href="#example-scenario"></a></h2>
<p>Say you review a document with <strong>10 true records</strong>:</p>
<p><strong>Model Performance:</strong> - Extracts 12 records - 7 are perfect (no edits needed) - 1 has 2 minor field edits - 1 has 1 major field edit (wrong species name) - 1 is missing (you add it) - 2 are hallucinations (you delete them)</p>
<p><strong>Each record has 8 fields total:</strong> - 4 unique fields (species names, dates) - 2 required fields (supporting sentences) - 2 optional fields (page number, publication year)</p>
<div class="section level3">
<h3 id="calculated-metrics">Calculated Metrics<a class="anchor" aria-label="anchor" href="#calculated-metrics"></a></h3>
<p><strong>Raw Counts:</strong> - model_extracted = 12 - human_added = 1 - deleted = 2 - records_with_edits = 2 - total_edits = 3 (2 minor + 1 major)</p>
<p><strong>Record Detection:</strong> - records_found = 12 - 2 = 10 - records_missed = 1 - records_hallucinated = 2 - detection_precision = 10/12 = 0.833 (83.3%) - detection_recall = 10/11 = 0.909 (90.9%) - perfect_record_rate = 7/10 = 0.70 (70%)</p>
<p><strong>Field-Level:</strong> - total_fields = 12 × 8 = 96 fields attempted - deleted_fields = 2 × 8 = 16 fields (in hallucinated records) - edited_fields = 3 fields corrected - correct_fields = 96 - 16 - 3 = 77 fields - field_precision = 77/96 = 0.802 (80.2%) - true_fields = (10 × 8) + (1 × 8) = 88 fields should exist - field_recall = 77/88 = 0.875 (87.5%)</p>
<p><strong>Edit Severity:</strong> - major_edits = 1 (the species name) - minor_edits = 2 (the two optional field edits) - major_edit_rate = 1/3 = 0.333 (33.3% of errors are major) - avg_edits_per_document = 3/1 = 3.0</p>
</div>
<div class="section level3">
<h3 id="interpretation">Interpretation<a class="anchor" aria-label="anchor" href="#interpretation"></a></h3>
<p>This model: - <strong>Good detection</strong>: Found 91% of records, only hallucinated 17% - <strong>Decent extraction</strong>: 80% of fields correct, giving partial credit for mostly-correct records - <strong>Room for improvement</strong>: 30% of found records need corrections - <strong>Encouraging error profile</strong>: Only 33% of errors are in critical identifying fields</p>
</div>
</div>
<div class="section level2">
<h2 id="usage">Usage<a class="anchor" aria-label="anchor" href="#usage"></a></h2>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/n8layman/ecoextract" class="external-link">ecoextract</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calculate accuracy for all verified documents</span></span>
<span><span class="va">accuracy</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/calculate_accuracy.html">calculate_accuracy</a></span><span class="op">(</span><span class="st">"my_database.db"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># View key metrics</span></span>
<span><span class="va">accuracy</span><span class="op">$</span><span class="va">detection_recall</span>    <span class="co"># Did we find the records?</span></span>
<span><span class="va">accuracy</span><span class="op">$</span><span class="va">field_precision</span>     <span class="co"># How accurate were the fields?</span></span>
<span><span class="va">accuracy</span><span class="op">$</span><span class="va">major_edit_rate</span>     <span class="co"># How serious were the errors?</span></span>
<span></span>
<span><span class="co"># Calculate accuracy for specific documents</span></span>
<span><span class="va">accuracy</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/calculate_accuracy.html">calculate_accuracy</a></span><span class="op">(</span><span class="st">"my_database.db"</span>, document_ids <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Use custom schema location</span></span>
<span><span class="va">accuracy</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/calculate_accuracy.html">calculate_accuracy</a></span><span class="op">(</span><span class="st">"my_database.db"</span>, schema_file <span class="op">=</span> <span class="st">"custom_schema.json"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="visualizing-accuracy">Visualizing Accuracy<a class="anchor" aria-label="anchor" href="#visualizing-accuracy"></a></h2>
<p>Accuracy visualizations (confusion matrices, field accuracy heatmaps) are available in the <a href="https://github.com/n8layman/ecoreview" class="external-link">ecoreview</a> Shiny app, which provides an interactive interface for reviewing extractions and viewing accuracy metrics.</p>
</div>
<div class="section level2">
<h2 id="design-decisions">Design Decisions<a class="anchor" aria-label="anchor" href="#design-decisions"></a></h2>
<div class="section level3">
<h3 id="why-field-level-instead-of-record-level">Why Field-Level Instead of Record-Level?<a class="anchor" aria-label="anchor" href="#why-field-level-instead-of-record-level"></a></h3>
<p><strong>Problem with record-level:</strong> A record with 1 wrong field out of 10 is treated the same as a completely wrong record (0% vs 90% correct).</p>
<p><strong>Field-level solution:</strong> Gives partial credit. The 90%-correct record contributes 9 correct fields and 1 incorrect field to the metrics.</p>
<p>This is more informative and better reflects the actual quality of extraction.</p>
</div>
<div class="section level3">
<h3 id="why-separate-detection-from-accuracy">Why Separate Detection from Accuracy?<a class="anchor" aria-label="anchor" href="#why-separate-detection-from-accuracy"></a></h3>
<p>Finding a record is different from extracting it correctly: - A model might find all records but extract many fields incorrectly (high recall, low field precision) - A model might only extract records it’s very confident about (low recall, high field precision)</p>
<p>Separating these dimensions helps diagnose where to improve.</p>
</div>
<div class="section level3">
<h3 id="why-classify-edit-severity">Why Classify Edit Severity?<a class="anchor" aria-label="anchor" href="#why-classify-edit-severity"></a></h3>
<p>Not all errors matter equally: - Wrong species name: Fundamentally changes what the record represents (major) - Wrong page number: Minor detail that doesn’t affect the science (minor)</p>
<p>Understanding error severity helps prioritize improvements.</p>
</div>
<div class="section level3">
<h3 id="why-average-edits-per-document">Why Average Edits Per Document?<a class="anchor" aria-label="anchor" href="#why-average-edits-per-document"></a></h3>
<p>This gives a sense of overall quality across the corpus: - avg_edits_per_document = 2: Model is quite accurate, minor cleanup needed - avg_edits_per_document = 50: Model needs significant improvement</p>
</div>
</div>
<div class="section level2">
<h2 id="technical-notes">Technical Notes<a class="anchor" aria-label="anchor" href="#technical-notes"></a></h2>
<div class="section level3">
<h3 id="assumptions">Assumptions<a class="anchor" aria-label="anchor" href="#assumptions"></a></h3>
<ul><li>All records have the same number of fields (defined by schema)</li>
<li>Nullable fields still count as fields for accuracy calculation</li>
<li>Edit severity requires schema with <code>x-unique-fields</code> and <code>required</code> specifications</li>
</ul></div>
<div class="section level3">
<h3 id="limitations">Limitations<a class="anchor" aria-label="anchor" href="#limitations"></a></h3>
<ul><li>No distinction between “empty → filled” vs “wrong → corrected” edits</li>
<li>Treats all edits within a severity class equally (no weighting by importance)</li>
<li>Assumes schema accurately reflects field importance via unique/required flags</li>
</ul></div>
<div class="section level3">
<h3 id="future-enhancements">Future Enhancements<a class="anchor" aria-label="anchor" href="#future-enhancements"></a></h3>
<p>Could add: - Edit type classification (addition vs correction vs deletion) - Per-document accuracy variance - Temporal trends (accuracy over time) - Confidence intervals for small sample sizes</p>
</div>
</div>
<div class="section level2">
<h2 id="related-documentation">Related Documentation<a class="anchor" aria-label="anchor" href="#related-documentation"></a></h2>
<ul><li>
<a href="https://github.com/n8layman/ecoreview" class="external-link">ecoreview README</a> - Review and edit records</li>
<li>
<a href="ecoextract/SCHEMA_GUIDE.html">SCHEMA_GUIDE.md</a> - Define custom schemas</li>
<li>
<a href="vignettes/ecoextract-workflow.Rmd">Vignette</a> - Complete workflow tutorial</li>
</ul></div>
<div class="section level2">
<h2 id="questions">Questions?<a class="anchor" aria-label="anchor" href="#questions"></a></h2>
<p>Open an issue on <a href="https://github.com/n8layman/ecoextract/issues" class="external-link">GitHub</a>.</p>
</div>
</div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Your Name.</p>
</div>

<div class="pkgdown-footer-right">
  <p>list(text = “EcoExtract - Structured ecological data extraction from scientific literature”)</p>
</div>

    </footer></div>





  
</body></html>

