[{"path":[]},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Understanding Accuracy Metrics in EcoExtract","text":"EcoExtract calculates accuracy analyzing human edits model-extracted records. review documents using ecoreview, edits tracked audit table (record_edits). calculate_accuracy() function analyzes edits provide comprehensive accuracy metrics.","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"philosophy-two-separate-questions","dir":"","previous_headings":"","what":"Philosophy: Two Separate Questions","title":"Understanding Accuracy Metrics in EcoExtract","text":"Traditional accuracy metrics treat extraction --nothing: record either “correct” “incorrect.” practice, extraction quality two independent dimensions: Record Detection: model find record, miss ? hallucinate records don’t exist? Field Accuracy: records model found, accurate individual fields? EcoExtract separates concerns give nuanced understanding model performance.","code":""},{"path":[]},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"ground-truth","dir":"","previous_headings":"Core Concepts","what":"Ground Truth","title":"Understanding Accuracy Metrics in EcoExtract","text":"Ground truth established human review: - Model extractions (records added_by_user = 0 NULL) - Human additions (records added_by_user = 1) indicate records model missed - Soft deletes (records deleted_by_user set) indicate hallucinations - Field edits (tracked record_edits table) indicate specific fields wrong documents marked reviewed (reviewed_at NULL) included accuracy calculations.","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"verified-documents","dir":"","previous_headings":"Core Concepts","what":"Verified Documents","title":"Understanding Accuracy Metrics in EcoExtract","text":"Accuracy calculated documents human-reviewed. ensures metrics reflect actual human judgment rather unverified model output.","code":""},{"path":[]},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"raw-counts","dir":"","previous_headings":"Metrics Explained","what":"Raw Counts","title":"Understanding Accuracy Metrics in EcoExtract","text":"fundamental measurements metrics derive: verified_documents: Number documents reviewed verified_records: Total records across verified documents model_extracted: Records model extracted (may include hallucinations) human_added: Records humans added model missed (false negatives) deleted: Records humans soft-deleted model hallucinated (false positives) records_with_edits: Records least one field corrected column_edits: Count edits per column (named vector)","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"record-detection-metrics","dir":"","previous_headings":"Metrics Explained","what":"Record Detection Metrics","title":"Understanding Accuracy Metrics in EcoExtract","text":"answer: “model find records?”","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"records-classification","dir":"","previous_headings":"Metrics Explained > Record Detection Metrics","what":"Records Classification","title":"Understanding Accuracy Metrics in EcoExtract","text":"True positives: Records model found (even imperfect) False negatives: Records exist model didn’t find False positives: Records model made don’t exist","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"detection-performance","dir":"","previous_headings":"Metrics Explained > Record Detection Metrics","what":"Detection Performance","title":"Understanding Accuracy Metrics in EcoExtract","text":"everything model extracted, fraction real (hallucinated)? High precision means hallucinations true records, fraction model find? High recall means missed records records model found, fraction zero errors? Measures extraction quality found records","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"field-level-metrics","dir":"","previous_headings":"Metrics Explained","what":"Field-Level Metrics","title":"Understanding Accuracy Metrics in EcoExtract","text":"answer: “accurate individual fields?” Field-level metrics provide partial credit. record 10 fields model got 9 correct, ’s credited 9 correct fields rather treated completely wrong.","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"field-calculations","dir":"","previous_headings":"Metrics Explained > Field-Level Metrics","what":"Field Calculations","title":"Understanding Accuracy Metrics in EcoExtract","text":"Total fields model attempted extract Fields extracted correctly deleted_fields = deleted × num_fields (fields deleted records wrong) edited_fields = sum(column_edits) (fields humans corrected)","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"field-performance","dir":"","previous_headings":"Metrics Explained > Field-Level Metrics","what":"Field Performance","title":"Understanding Accuracy Metrics in EcoExtract","text":"fields model extracted, fraction correct? true_fields = (records_found × num_fields) + (human_added × num_fields) true fields exist, fraction extract correctly? 2 × precision × recall / (precision + recall) Balanced measure combining metrics","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"per-column-accuracy","dir":"","previous_headings":"Metrics Explained","what":"Per-Column Accuracy","title":"Understanding Accuracy Metrics in EcoExtract","text":"Formula: 1 - (column_edits / model_extracted) Example: 100 records extracted “species” field edited 15 times, species accuracy = 0.85 Shows fields hardest model extract Helps identify improve prompts schemas","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"edit-severity-metrics","dir":"","previous_headings":"Metrics Explained","what":"Edit Severity Metrics","title":"Understanding Accuracy Metrics in EcoExtract","text":"edits equal. EcoExtract classifies edits based schema:","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"major-vs-minor-edits","dir":"","previous_headings":"Metrics Explained > Edit Severity Metrics","what":"Major vs Minor Edits","title":"Understanding Accuracy Metrics in EcoExtract","text":"Major edits affect fields identify required record: - Unique fields (x-unique-fields schema): Fields identify record - Example: species names, dates, locations distinguish one record another - Required fields (required schema): Fields mandatory record validity - Example: supporting sentences, organisms_identifiable Minor edits affect optional descriptive fields: - Fields provide additional detail aren’t core record identity - Example: publication_year, page_number","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"severity-metrics","dir":"","previous_headings":"Metrics Explained > Edit Severity Metrics","what":"Severity Metrics","title":"Understanding Accuracy Metrics in EcoExtract","text":"major_edits: Count edits unique required fields minor_edits: Count edits fields Closer 1.0 means errors critical fields Closer 0.0 means errors minor details average, many field corrections per document? Lower better","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"example-scenario","dir":"","previous_headings":"","what":"Example Scenario","title":"Understanding Accuracy Metrics in EcoExtract","text":"Say review document 10 true records: Model Performance: - Extracts 12 records - 7 perfect (edits needed) - 1 2 minor field edits - 1 1 major field edit (wrong species name) - 1 missing (add ) - 2 hallucinations (delete ) record 8 fields total: - 4 unique fields (species names, dates) - 2 required fields (supporting sentences) - 2 optional fields (page number, publication year)","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"calculated-metrics","dir":"","previous_headings":"Example Scenario","what":"Calculated Metrics","title":"Understanding Accuracy Metrics in EcoExtract","text":"Raw Counts: - model_extracted = 12 - human_added = 1 - deleted = 2 - records_with_edits = 2 - total_edits = 3 (2 minor + 1 major) Record Detection: - records_found = 12 - 2 = 10 - records_missed = 1 - records_hallucinated = 2 - detection_precision = 10/12 = 0.833 (83.3%) - detection_recall = 10/11 = 0.909 (90.9%) - perfect_record_rate = 7/10 = 0.70 (70%) Field-Level: - total_fields = 12 × 8 = 96 fields attempted - deleted_fields = 2 × 8 = 16 fields (hallucinated records) - edited_fields = 3 fields corrected - correct_fields = 96 - 16 - 3 = 77 fields - field_precision = 77/96 = 0.802 (80.2%) - true_fields = (10 × 8) + (1 × 8) = 88 fields exist - field_recall = 77/88 = 0.875 (87.5%) Edit Severity: - major_edits = 1 (species name) - minor_edits = 2 (two optional field edits) - major_edit_rate = 1/3 = 0.333 (33.3% errors major) - avg_edits_per_document = 3/1 = 3.0","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"interpretation","dir":"","previous_headings":"Example Scenario","what":"Interpretation","title":"Understanding Accuracy Metrics in EcoExtract","text":"model: - Good detection: Found 91% records, hallucinated 17% - Decent extraction: 80% fields correct, giving partial credit mostly-correct records - Room improvement: 30% found records need corrections - Encouraging error profile: 33% errors critical identifying fields","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Understanding Accuracy Metrics in EcoExtract","text":"","code":"library(ecoextract)  # Calculate accuracy for all verified documents accuracy <- calculate_accuracy(\"my_database.db\")  # View key metrics accuracy$detection_recall    # Did we find the records? accuracy$field_precision     # How accurate were the fields? accuracy$major_edit_rate     # How serious were the errors?  # Calculate accuracy for specific documents accuracy <- calculate_accuracy(\"my_database.db\", document_ids = c(1, 2, 3))  # Use custom schema location accuracy <- calculate_accuracy(\"my_database.db\", schema_file = \"custom_schema.json\")"},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"visualizing-accuracy","dir":"","previous_headings":"","what":"Visualizing Accuracy","title":"Understanding Accuracy Metrics in EcoExtract","text":"Accuracy visualizations (confusion matrices, field accuracy heatmaps) available ecoreview Shiny app, provides interactive interface reviewing extractions viewing accuracy metrics.","code":""},{"path":[]},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"why-field-level-instead-of-record-level","dir":"","previous_headings":"Design Decisions","what":"Why Field-Level Instead of Record-Level?","title":"Understanding Accuracy Metrics in EcoExtract","text":"Problem record-level: record 1 wrong field 10 treated completely wrong record (0% vs 90% correct). Field-level solution: Gives partial credit. 90%-correct record contributes 9 correct fields 1 incorrect field metrics. informative better reflects actual quality extraction.","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"why-separate-detection-from-accuracy","dir":"","previous_headings":"Design Decisions","what":"Why Separate Detection from Accuracy?","title":"Understanding Accuracy Metrics in EcoExtract","text":"Finding record different extracting correctly: - model might find records extract many fields incorrectly (high recall, low field precision) - model might extract records ’s confident (low recall, high field precision) Separating dimensions helps diagnose improve.","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"why-classify-edit-severity","dir":"","previous_headings":"Design Decisions","what":"Why Classify Edit Severity?","title":"Understanding Accuracy Metrics in EcoExtract","text":"errors matter equally: - Wrong species name: Fundamentally changes record represents (major) - Wrong page number: Minor detail doesn’t affect science (minor) Understanding error severity helps prioritize improvements.","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"why-average-edits-per-document","dir":"","previous_headings":"Design Decisions","what":"Why Average Edits Per Document?","title":"Understanding Accuracy Metrics in EcoExtract","text":"gives sense overall quality across corpus: - avg_edits_per_document = 2: Model quite accurate, minor cleanup needed - avg_edits_per_document = 50: Model needs significant improvement","code":""},{"path":[]},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"assumptions","dir":"","previous_headings":"Technical Notes","what":"Assumptions","title":"Understanding Accuracy Metrics in EcoExtract","text":"records number fields (defined schema) Nullable fields still count fields accuracy calculation Edit severity requires schema x-unique-fields required specifications","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"limitations","dir":"","previous_headings":"Technical Notes","what":"Limitations","title":"Understanding Accuracy Metrics in EcoExtract","text":"distinction “empty → filled” vs “wrong → corrected” edits Treats edits within severity class equally (weighting importance) Assumes schema accurately reflects field importance via unique/required flags","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"future-enhancements","dir":"","previous_headings":"Technical Notes","what":"Future Enhancements","title":"Understanding Accuracy Metrics in EcoExtract","text":"add: - Edit type classification (addition vs correction vs deletion) - Per-document accuracy variance - Temporal trends (accuracy time) - Confidence intervals small sample sizes","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"related-documentation","dir":"","previous_headings":"","what":"Related Documentation","title":"Understanding Accuracy Metrics in EcoExtract","text":"ecoreview README - Review edit records SCHEMA_GUIDE.md - Define custom schemas Vignette - Complete workflow tutorial","code":""},{"path":"https://n8layman.github.io/ecoextract/ACCURACY.html","id":"questions","dir":"","previous_headings":"","what":"Questions?","title":"Understanding Accuracy Metrics in EcoExtract","text":"Open issue GitHub.","code":""},{"path":"https://n8layman.github.io/ecoextract/BRANCH_PROTECTION.html","id":null,"dir":"","previous_headings":"","what":"GitHub Branch Protection Setup","title":"GitHub Branch Protection Setup","text":"document explains configure GitHub require tests pass approval merging PRs using GitHub Rulesets (modern approach 2024-2025).","code":""},{"path":"https://n8layman.github.io/ecoextract/BRANCH_PROTECTION.html","id":"id_1-add-repository-secrets","dir":"","previous_headings":"","what":"1. Add Repository Secrets","title":"GitHub Branch Protection Setup","text":"Go repository GitHub Click Settings → Secrets variables → Actions Click New repository secret Name: ANTHROPIC_API_KEY, Value: Anthropic API key Name: MISTRAL_API_KEY, Value: Mistral API key","code":""},{"path":[]},{"path":"https://n8layman.github.io/ecoextract/BRANCH_PROTECTION.html","id":"step-1-navigate-to-rulesets","dir":"","previous_headings":"2. Create a Branch Ruleset","what":"Step 1: Navigate to Rulesets","title":"GitHub Branch Protection Setup","text":"Go repository Settings left sidebar: Rules → Rulesets Click New ruleset → New branch ruleset","code":""},{"path":"https://n8layman.github.io/ecoextract/BRANCH_PROTECTION.html","id":"step-2-configure-ruleset-basics","dir":"","previous_headings":"2. Create a Branch Ruleset","what":"Step 2: Configure Ruleset Basics","title":"GitHub Branch Protection Setup","text":"Ruleset Name: Enter “Protect main branch” Enforcement status: Select Active","code":""},{"path":"https://n8layman.github.io/ecoextract/BRANCH_PROTECTION.html","id":"step-3-set-target-branches","dir":"","previous_headings":"2. Create a Branch Ruleset","what":"Step 3: Set Target Branches","title":"GitHub Branch Protection Setup","text":"Click Add target Select Include pattern Enter pattern: main (master ’s default branch)","code":""},{"path":"https://n8layman.github.io/ecoextract/BRANCH_PROTECTION.html","id":"step-4-configure-branch-protections","dir":"","previous_headings":"2. Create a Branch Ruleset","what":"Step 4: Configure Branch Protections","title":"GitHub Branch Protection Setup","text":"Required settings: Check box Set Required approvals 1 ensures must approve PRs can merge Check box Click Add checks search box, type: R-CMD-check Click + icon add Check Require branches date merging Optional recommended: - Require conversation resolution merging - Ensures PR comments addressed - Require signed commits - Extra security - Require linear history - Keeps git history clean (merge commits)","code":""},{"path":"https://n8layman.github.io/ecoextract/BRANCH_PROTECTION.html","id":"step-5-save-ruleset","dir":"","previous_headings":"2. Create a Branch Ruleset","what":"Step 5: Save Ruleset","title":"GitHub Branch Protection Setup","text":"Click Create Ruleset now active enforced immediately","code":""},{"path":"https://n8layman.github.io/ecoextract/BRANCH_PROTECTION.html","id":"id_3-workflow-behavior","dir":"","previous_headings":"","what":"3. Workflow Behavior","title":"GitHub Branch Protection Setup","text":"workflow (R-CMD-check.yaml) runs: - : Pull request opened updated targeting main - : Runs full test suite real API calls using repository secrets - Result: PR can merge : 1. tests pass (green checkmark) 2. approve PR","code":""},{"path":[]},{"path":"https://n8layman.github.io/ecoextract/BRANCH_PROTECTION.html","id":"creating-a-pr","dir":"","previous_headings":"4. Working with Protected Branches","what":"Creating a PR","title":"GitHub Branch Protection Setup","text":"","code":"# Create feature branch git checkout -b feature/my-feature  # Make changes and commit git add . git commit -m \"Add new feature\"  # Push to GitHub git push origin feature/my-feature"},{"path":"https://n8layman.github.io/ecoextract/BRANCH_PROTECTION.html","id":"on-github","dir":"","previous_headings":"4. Working with Protected Branches","what":"On GitHub","title":"GitHub Branch Protection Setup","text":"Open pull request main Tests run automatically Green checkmark = tests passed Red X = tests failed (must fix merging) review approve PR tests pass approve, “Merge pull request” becomes available","code":""},{"path":"https://n8layman.github.io/ecoextract/BRANCH_PROTECTION.html","id":"merging-requirements-checklist","dir":"","previous_headings":"4. Working with Protected Branches","what":"Merging Requirements Checklist","title":"GitHub Branch Protection Setup","text":"Tests passed (R-CMD-check green) approved PR conversations resolved (enabled) Branch date (enabled) requirements met can PR merged.","code":""},{"path":"https://n8layman.github.io/ecoextract/BRANCH_PROTECTION.html","id":"id_5-viewing-test-results","dir":"","previous_headings":"","what":"5. Viewing Test Results","title":"GitHub Branch Protection Setup","text":"PR page, click Checks tab See detailed test output Click Details next R-CMD-check see full logs Review failures fix re-pushing","code":""},{"path":"https://n8layman.github.io/ecoextract/BRANCH_PROTECTION.html","id":"id_6-local-testing-before-pushing","dir":"","previous_headings":"","what":"6. Local Testing Before Pushing","title":"GitHub Branch Protection Setup","text":"avoid failing tests CI: push/create PR local tests pass.","code":"# Load your .env file with API keys ecoextract::load_env_file()  # Run tests locally devtools::test()  # Check package devtools::check()"},{"path":"https://n8layman.github.io/ecoextract/BRANCH_PROTECTION.html","id":"id_7-first-time-setup","dir":"","previous_headings":"","what":"7. First-Time Setup","title":"GitHub Branch Protection Setup","text":"creating ruleset: Create test PR trigger workflow first time workflow runs, R-CMD-check appear status checks list Edit ruleset needed add check (wasn’t available initially)","code":""},{"path":"https://n8layman.github.io/ecoextract/BRANCH_PROTECTION.html","id":"id_8-troubleshooting","dir":"","previous_headings":"","what":"8. Troubleshooting","title":"GitHub Branch Protection Setup","text":"“Required status check ‘R-CMD-check’ found” - workflow needs run least appears - Create test PR push commit trigger Tests failing CI passing locally - Check repository secrets set correctly - Verify secrets expired - Check CI logs specific error messages Can’t merge even though tests passed - Make sure ’ve approved PR (approval required) - Check required status checks green - Verify branch date option enabled","code":""},{"path":"https://n8layman.github.io/ecoextract/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to EcoExtract","title":"Contributing to EcoExtract","text":"Interested contributing? Reach opening issue first – ’d like coordinate start work.","code":""},{"path":"https://n8layman.github.io/ecoextract/CONTRIBUTING.html","id":"rules","dir":"","previous_headings":"","what":"Rules","title":"Contributing to EcoExtract","text":"changes via pull request (squash-merged main) Run devtools::test() devtools::check() submitting create PR work complete – push triggers CI integration tests consume API credits Use native pipe |>, tidyverse style, roxygen2 docs Tests must schema-agnostic (see tests/TESTING.md) Use withr cleanup tests","code":""},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program–make sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers’ authors’ protection, GPL clearly explains warranty free software. users’ authors’ sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users’ freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"“License” refers version 3 GNU General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":"id_13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"https://n8layman.github.io/ecoextract/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program terminal interaction, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, program’s commands might different; GUI interface, use “box”. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU GPL, see <http://www.gnu.org/licenses/>. GNU General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License. first, please read <http://www.gnu.org/philosophy/--lgpl.html>.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>. <program>  Copyright (C) <year>  <name of author> This program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'. This is free software, and you are welcome to redistribute it under certain conditions; type 'show c' for details."},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Configuring EcoExtract for Your Data","text":"EcoExtract domain-agnostic works JSON schema define. guide teaches customize EcoExtract extract exactly data need research domain.","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"what-youll-learn","dir":"Articles","previous_headings":"Introduction","what":"What You’ll Learn","title":"Configuring EcoExtract for Your Data","text":"create custom configuration files define data schema (fields extract) write extraction prompts (extract fields) use array fields multi-valued data work required vs optional fields test iterate configuration","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"configuration-files","dir":"Articles","previous_headings":"Introduction","what":"Configuration Files","title":"Configuring EcoExtract for Your Data","text":"EcoExtract’s extraction behavior controlled two key configuration files: schema.json - Defines structure extracted data (fields, types) extraction_prompt.md - Provides instructions LLM (extract, edge cases) ’s also optional refinement prompt: refinement_prompt.md - Instructions refinement step (validating enhancing data)","code":""},{"path":[]},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"schema-basics","dir":"Articles","previous_headings":"Understanding the Schema","what":"Schema Basics","title":"Configuring EcoExtract for Your Data","text":"schema JSON file defines: fields extracted data types field fields required vs optional validate extracted data fields uniquely identify records","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"required-structure","dir":"Articles","previous_headings":"Understanding the Schema","what":"Required Structure","title":"Configuring EcoExtract for Your Data","text":"schema must follow structure: Key requirements: Top-level must records property (array objects) field type description Every object must \"additionalProperties\": false properties must listed required – use nullable types ([\"string\", \"null\"]) optional fields Use x-unique-fields identify fields define uniqueness (used deduplication accuracy) See Cross-Provider Compatibility full details","code":"{   \"$schema\": \"http://json-schema.org/draft-07/schema#\",   \"title\": \"Your Domain Schema Title\",   \"type\": \"object\",   \"additionalProperties\": false,   \"required\": [\"records\"],   \"properties\": {     \"records\": {       \"type\": \"array\",       \"description\": \"Array of extracted records\",       \"items\": {         \"type\": \"object\",         \"additionalProperties\": false,         \"required\": [\"field1\", \"field2\", \"optional_field\"],         \"properties\": {           \"field1\": {             \"type\": \"string\",             \"description\": \"A required string field\"           },           \"field2\": {             \"type\": \"integer\",             \"description\": \"A required integer field\"           },           \"optional_field\": {             \"type\": [\"string\", \"null\"],             \"description\": \"Optional - returns null when not available\"           }         },         \"x-unique-fields\": [\"field1\", \"field2\"]       }     }   } }"},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"field-types","dir":"Articles","previous_headings":"Understanding the Schema","what":"Field Types","title":"Configuring EcoExtract for Your Data","text":"schema supports standard JSON Schema types:","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"reserved-system-field","dir":"Articles","previous_headings":"Understanding the Schema","what":"Reserved System Field","title":"Configuring EcoExtract for Your Data","text":"⚠️ IMPORTANT: record_id reserved system field: Format: AuthorYear-(e.g., Smith2020-o1) Purpose: Unique identifier record Generated: Automatically system include schema - managed internally","code":""},{"path":[]},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"step-1-initialize-configuration","dir":"Articles","previous_headings":"Creating Custom Configuration","what":"Step 1: Initialize Configuration","title":"Configuring EcoExtract for Your Data","text":"Create template configuration files project: creates: ecoextract/SCHEMA_GUIDE.md - Read first! ecoextract/schema.json - Template schema customize ecoextract/extraction_prompt.md - Template prompt customize","code":"library(ecoextract)  # Creates ecoextract/ directory with template files init_ecoextract()"},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"step-2-edit-the-schema","dir":"Articles","previous_headings":"Creating Custom Configuration","what":"Step 2: Edit the Schema","title":"Configuring EcoExtract for Your Data","text":"Open ecoextract/schema.json customize domain.","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"example-1-simple-schema-disease-outbreaks","dir":"Articles","previous_headings":"Creating Custom Configuration > Step 2: Edit the Schema","what":"Example 1: Simple Schema (Disease Outbreaks)","title":"Configuring EcoExtract for Your Data","text":"","code":"{   \"$schema\": \"http://json-schema.org/draft-07/schema#\",   \"title\": \"Disease Outbreak Schema\",   \"description\": \"Schema for extracting disease outbreak data from epidemiological literature\",   \"type\": \"object\",   \"additionalProperties\": false,   \"required\": [\"records\"],   \"properties\": {     \"records\": {       \"type\": \"array\",       \"description\": \"Array of disease outbreak records\",       \"items\": {         \"type\": \"object\",         \"additionalProperties\": false,         \"required\": [           \"disease_name\",           \"location\",           \"year\",           \"cases\",           \"deaths\",           \"all_supporting_source_sentences\"         ],         \"properties\": {           \"disease_name\": {             \"type\": \"string\",             \"description\": \"Name of the disease (e.g., 'COVID-19', 'Influenza A(H1N1)')\"           },           \"location\": {             \"type\": \"string\",             \"description\": \"Geographic location of outbreak (city, region, or country)\"           },           \"year\": {             \"type\": \"integer\",             \"description\": \"Year the outbreak occurred\"           },           \"cases\": {             \"type\": [\"integer\", \"null\"],             \"description\": \"Total number of cases reported (null if not stated)\"           },           \"deaths\": {             \"type\": [\"integer\", \"null\"],             \"description\": \"Total number of deaths reported (null if not stated)\"           },           \"all_supporting_source_sentences\": {             \"type\": \"array\",             \"description\": \"All sentences from the document that support this record\",             \"items\": {               \"type\": \"string\",               \"description\": \"Individual supporting sentence\"             }           }         },         \"x-unique-fields\": [\"disease_name\", \"location\", \"year\"]       }     }   } }"},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"example-2-complex-schema-with-arrays-host-pathogen-interactions","dir":"Articles","previous_headings":"Creating Custom Configuration > Step 2: Edit the Schema","what":"Example 2: Complex Schema with Arrays (Host-Pathogen Interactions)","title":"Configuring EcoExtract for Your Data","text":"","code":"{   \"$schema\": \"http://json-schema.org/draft-07/schema#\",   \"title\": \"Host-Pathogen Interaction Schema\",   \"type\": \"object\",   \"additionalProperties\": false,   \"required\": [\"records\"],   \"properties\": {     \"records\": {       \"type\": \"array\",       \"items\": {         \"type\": \"object\",         \"additionalProperties\": false,         \"required\": [           \"Pathogen_Name\",           \"Host_Name\",           \"Detection_Method\",           \"Sample_type\",           \"Location\",           \"all_supporting_source_sentences\",           \"Confidence_Score\"         ],         \"properties\": {           \"Pathogen_Name\": {             \"type\": \"string\",             \"description\": \"Scientific name of the pathogen\"           },           \"Host_Name\": {             \"type\": \"string\",             \"description\": \"Scientific name of the host organism\"           },           \"Detection_Method\": {             \"type\": \"array\",             \"description\": \"Methods used to detect the pathogen (can be multiple)\",             \"items\": {               \"type\": \"string\",               \"description\": \"Individual detection method (e.g., 'PCR', 'culture', 'serology')\"             }           },           \"Sample_type\": {             \"type\": \"array\",             \"description\": \"Types of samples collected (empty array if not mentioned)\",             \"items\": {               \"type\": \"string\",               \"description\": \"Individual sample type (e.g., 'blood', 'tissue', 'fecal swab')\"             }           },           \"Location\": {             \"type\": [\"string\", \"null\"],             \"description\": \"Geographic location where interaction was documented\"           },           \"all_supporting_source_sentences\": {             \"type\": \"array\",             \"description\": \"All sentences from document supporting this record\",             \"items\": {               \"type\": \"string\"             }           },           \"Confidence_Score\": {             \"type\": [\"integer\", \"null\"],             \"description\": \"Confidence in extraction accuracy (1-5 scale)\"           }         },         \"x-unique-fields\": [\"Pathogen_Name\", \"Host_Name\"]       }     }   } }"},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"step-3-edit-the-extraction-prompt","dir":"Articles","previous_headings":"Creating Custom Configuration","what":"Step 3: Edit the Extraction Prompt","title":"Configuring EcoExtract for Your Data","text":"Open ecoextract/extraction_prompt.md customize instructions domain.","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"prompt-structure","dir":"Articles","previous_headings":"Creating Custom Configuration > Step 3: Edit the Extraction Prompt","what":"Prompt Structure","title":"Configuring EcoExtract for Your Data","text":"good extraction prompt include: Task description - ’re extracting Field definitions - Clear explanation field Extraction rules - create separate records Edge case handling - ambiguous cases Quality guidelines - Standards completeness accuracy","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"example-prompt-disease-outbreaks","dir":"Articles","previous_headings":"Creating Custom Configuration > Step 3: Edit the Extraction Prompt","what":"Example Prompt (Disease Outbreaks)","title":"Configuring EcoExtract for Your Data","text":"","code":"# Task  Extract all disease outbreak events mentioned in this epidemiological document. Each outbreak should be a separate record.  # Field Definitions  - **disease_name**: The specific disease or pathogen name. Use the most specific   name mentioned (e.g., \"Influenza A(H1N1)\" not just \"Influenza\").  - **location**: Geographic location where outbreak occurred. Use the most specific   location mentioned (city > region > country).  - **year**: Year the outbreak occurred (4-digit year).  - **cases**: Total number of confirmed or suspected cases. Extract only if explicitly   stated.  - **deaths**: Total number of deaths attributed to the outbreak. Extract only if   explicitly stated.  - **all_supporting_source_sentences**: Include every sentence that provides   information about this outbreak. Include the full sentence verbatim.  # Extraction Rules  ## One Record Per Outbreak  Create separate records for: - Different diseases in the same location - Same disease in different locations - Same disease in same location but different time periods  ## Handling Uncertainty  - If year is not explicitly stated, try to infer from context - If location is ambiguous, use the most likely interpretation - If case/death counts are ranges, use the midpoint - Mark low-confidence extractions with Confidence_Score = 2 or 3  ## Multiple Mentions  If an outbreak is mentioned multiple times, create ONE record and include all supporting sentences.  # Quality Standards  - Prioritize completeness: extract all outbreaks mentioned - Include all supporting sentences for each record - Be precise with location and disease names - Don't hallucinate: only extract what's explicitly stated or clearly implied"},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"step-4-test-your-configuration","dir":"Articles","previous_headings":"Creating Custom Configuration","what":"Step 4: Test Your Configuration","title":"Configuring EcoExtract for Your Data","text":"","code":"# Process a test document with your custom configuration results <- process_documents(   pdf_path = \"test_paper.pdf\",   db_conn = \"test.db\" )  # Retrieve and inspect results records <- get_records(db_conn = \"test.db\") View(records)  # Check what was extracted names(records)  # Should match your schema fields str(records)    # Check data types"},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"step-5-iterate-and-refine","dir":"Articles","previous_headings":"Creating Custom Configuration","what":"Step 5: Iterate and Refine","title":"Configuring EcoExtract for Your Data","text":"","code":"# Common iterations: # 1. Adjust field descriptions in schema # 2. Add edge case handling in prompt # 3. Reprocess with force flag # 4. Review results in ecoreview app  # Force reprocess after changing schema/prompt results <- process_documents(   pdf_path = \"test_paper.pdf\",   db_conn = \"test.db\",   force_reprocess_extraction = TRUE )"},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"working-with-array-fields","dir":"Articles","previous_headings":"","what":"Working with Array Fields","title":"Configuring EcoExtract for Your Data","text":"Array fields let capture multi-valued data like: - Multiple detection methods per observation - Multiple sample types - Multiple supporting sentences - Multiple geographic locations","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"defining-array-fields","dir":"Articles","previous_headings":"Working with Array Fields","what":"Defining Array Fields","title":"Configuring EcoExtract for Your Data","text":"","code":"{   \"detection_methods\": {     \"type\": \"array\",     \"description\": \"All methods used to detect the organism\",     \"items\": {       \"type\": \"string\",       \"description\": \"Individual detection method (e.g., 'PCR', 'culture', 'ELISA')\"     }   } }"},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"instructing-the-llm-on-arrays","dir":"Articles","previous_headings":"Working with Array Fields","what":"Instructing the LLM on Arrays","title":"Configuring EcoExtract for Your Data","text":"extraction prompt, clearly explain use arrays:","code":"## detection_methods (array)  List ALL methods mentioned for detecting this organism. Each method should be a separate array element.  Examples: - If text says \"detected by PCR and culture\" → [\"PCR\", \"culture\"] - If text says \"PCR-positive samples\" → [\"PCR\"] - If text says \"antibodies detected via ELISA\" → [\"ELISA\"]  Common methods: PCR, RT-PCR, qPCR, culture, serology, ELISA, Western blot, microscopy, sequencing, antigen detection"},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"retrieving-array-data-in-r","dir":"Articles","previous_headings":"Working with Array Fields","what":"Retrieving Array Data in R","title":"Configuring EcoExtract for Your Data","text":"","code":"library(dplyr) library(tidyr)  # Get records with array fields records <- get_records(db_conn = \"records.db\")  # Array fields are stored as lists class(records$detection_methods)  # \"list\"  # Unnest to see individual values records |>   unnest(detection_methods) |>   select(record_id, detection_methods)  # Count frequency of each method records |>   unnest(detection_methods) |>   count(detection_methods, sort = TRUE)"},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"best-practices-for-arrays","dir":"Articles","previous_headings":"Working with Array Fields","what":"Best Practices for Arrays","title":"Configuring EcoExtract for Your Data","text":"explicit prompts - Show examples single vs multiple values Use consistent terminology - Define controlled vocabulary possible Always include descriptions - Help LLM understand belongs array Test edge cases - Papers 1 value, 5 values, 0 values","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"structural-vs-data-requirements","dir":"Articles","previous_headings":"","what":"Structural vs Data Requirements","title":"Configuring EcoExtract for Your Data","text":"required array JSON Schema serves two purposes important distinguish:","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"structural-requirement-for-openai","dir":"Articles","previous_headings":"Structural vs Data Requirements","what":"Structural Requirement (for OpenAI)","title":"Configuring EcoExtract for Your Data","text":"OpenAI’s structured outputs API requires every property listed required. structural constraint – ensures model always returns fields response. mean every field must contain meaningful data.","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"data-requirements-for-your-domain","dir":"Articles","previous_headings":"Structural vs Data Requirements","what":"Data Requirements (for your domain)","title":"Configuring EcoExtract for Your Data","text":"type field controls whether must contain data:","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"example","dir":"Articles","previous_headings":"Structural vs Data Requirements","what":"Example","title":"Configuring EcoExtract for Your Data","text":"five fields structurally required (required), pathogen_name host_name must data. location can null, detection_methods can []. Important: use [\"array\", \"null\"] optional arrays. Use plain \"array\" instruct model return [] data available.","code":"{   \"items\": {     \"type\": \"object\",     \"additionalProperties\": false,     \"required\": [       \"pathogen_name\",       \"host_name\",       \"location\",       \"detection_methods\",       \"all_supporting_source_sentences\"     ],     \"properties\": {       \"pathogen_name\": { \"type\": \"string\", \"description\": \"Always present\" },       \"host_name\": { \"type\": \"string\", \"description\": \"Always present\" },       \"location\": { \"type\": [\"string\", \"null\"], \"description\": \"Null if not mentioned\" },       \"detection_methods\": { \"type\": \"array\", \"items\": { \"type\": \"string\" }, \"description\": \"Empty array if not mentioned\" },       \"all_supporting_source_sentences\": { \"type\": \"array\", \"items\": { \"type\": \"string\" }, \"description\": \"Supporting evidence\" }     }   } }"},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"impact-on-downstream-processing","dir":"Articles","previous_headings":"Structural vs Data Requirements","what":"Impact on Downstream Processing","title":"Configuring EcoExtract for Your Data","text":"Accuracy calculation - Edits non-nullable fields “major edits” Deduplication - Fields x-unique-fields generally non-nullable Database constraints - Non-nullable fields get NULL constraints; nullable fields allow NULL","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"guidelines","dir":"Articles","previous_headings":"Structural vs Data Requirements","what":"Guidelines","title":"Configuring EcoExtract for Your Data","text":"Non-nullable (truly required): - Core identifying information (organism names) - Evidence fields (all_supporting_source_sentences) - Fields needed deduplication Nullable (optional): - Descriptive details may always present - Quantitative measurements (counts, concentrations) - Contextual information (dates, locations, page numbers) Array fields (may empty): - Multi-valued fields like detection methods, sample types, vector names - Use field description indicate empty arrays acceptable","code":""},{"path":[]},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"defining-unique-fields","dir":"Articles","previous_headings":"Unique Fields and Deduplication","what":"Defining Unique Fields","title":"Configuring EcoExtract for Your Data","text":"Use x-unique-fields specify fields define record uniqueness:","code":"{   \"items\": {     \"type\": \"object\",     \"additionalProperties\": false,     \"required\": [\"pathogen_name\", \"host_name\", \"location\", \"sample_date\"],     \"properties\": {       \"pathogen_name\": { \"type\": \"string\", \"description\": \"Pathogen name\" },       \"host_name\": { \"type\": \"string\", \"description\": \"Host name\" },       \"location\": { \"type\": [\"string\", \"null\"], \"description\": \"Location\" },       \"sample_date\": { \"type\": [\"string\", \"null\"], \"description\": \"Date\" }     },     \"x-unique-fields\": [\"pathogen_name\", \"host_name\", \"location\"]   } }"},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"how-unique-fields-are-used","dir":"Articles","previous_headings":"Unique Fields and Deduplication","what":"How Unique Fields Are Used","title":"Configuring EcoExtract for Your Data","text":"Deduplication - Records unique field values considered duplicates Accuracy metrics - Edits unique fields classified “major edits” Review workflow - Helps identify truly distinct records","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"choosing-unique-fields","dir":"Articles","previous_headings":"Unique Fields and Deduplication","what":"Choosing Unique Fields","title":"Configuring EcoExtract for Your Data","text":"Good unique field combinations: - Species interactions: species1 + species2 + interaction_type - Disease outbreaks: disease + location + year - Chemical measurements: chemical + sample_location + date - Observations: organism + location + date + observer","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"cross-provider-compatibility","dir":"Articles","previous_headings":"","what":"Cross-Provider Compatibility","title":"Configuring EcoExtract for Your Data","text":"ecoextract supports multiple LLM providers (Anthropic Claude, OpenAI GPT, Mistral). Schemas must follow OpenAI’s structured output requirements work across providers: additionalProperties: false every object definition properties listed required – every field properties must also appear required Nullable types optional fields – use \"type\": [\"string\", \"null\"] instead omitting required. model returns null data available minimum/maximum constraints – put range info description instead (e.g., \"description\": \"Confidence score (1-5 scale)\") $ref – inline definitions Max 100 properties, 5 nesting levels rules safe providers – Claude Mistral handle without issues. full details, see OpenAI’s Structured Outputs documentation.","code":""},{"path":[]},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"configuration-priority","dir":"Articles","previous_headings":"Advanced Configuration","what":"Configuration Priority","title":"Configuring EcoExtract for Your Data","text":"EcoExtract looks configuration files order: Explicit parameter - Files specify directly function calls Project ecoextract/ directory - ecoextract/schema.json, etc. Working directory prefix - ecoextract_schema.json, etc. Package defaults - Built-example schema","code":"# Uses ecoextract/schema.json automatically process_documents(\"pdfs/\", \"records.db\")  # Specify custom path explicitly process_documents(   pdf_path = \"pdfs/\",   db_conn = \"records.db\",   schema_file = \"custom/my_schema.json\",   extraction_prompt_file = \"custom/my_prompt.md\" )"},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"refinement-prompts","dir":"Articles","previous_headings":"Advanced Configuration","what":"Refinement Prompts","title":"Configuring EcoExtract for Your Data","text":"refinement step validates enhances extracted data. Customize refinement prompt domain-specific validation: good refinement prompt : - Validate field values known standards - Check logical consistency - Enhance incomplete records using context - Resolve ambiguities extraction step","code":"# Run with custom refinement prompt process_documents(   pdf_path = \"pdfs/\",   db_conn = \"records.db\",   run_refinement = TRUE,   refinement_prompt_file = \"ecoextract/refinement_prompt.md\" )"},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"multiple-schemas-for-different-projects","dir":"Articles","previous_headings":"Advanced Configuration","what":"Multiple Schemas for Different Projects","title":"Configuring EcoExtract for Your Data","text":"","code":"# Project 1: Disease outbreaks process_documents(   \"disease_pdfs/\",   \"disease.db\",   schema_file = \"schemas/disease_schema.json\",   extraction_prompt_file = \"prompts/disease_prompt.md\" )  # Project 2: Species interactions process_documents(   \"species_pdfs/\",   \"species.db\",   schema_file = \"schemas/species_schema.json\",   extraction_prompt_file = \"prompts/species_prompt.md\" )"},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"complete-example-building-a-custom-schema","dir":"Articles","previous_headings":"","what":"Complete Example: Building a Custom Schema","title":"Configuring EcoExtract for Your Data","text":"Let’s walk creating schema extracting plant-pollinator interactions:","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"define-your-domain","dir":"Articles","previous_headings":"Complete Example: Building a Custom Schema","what":"1. Define Your Domain","title":"Configuring EcoExtract for Your Data","text":"information need? - Plant species (scientific name) - Pollinator species (scientific name) - Interaction type (pollination, nectar robbing, etc.) - Location (observed) - Date/season (observed) - Supporting evidence","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"create-the-schema","dir":"Articles","previous_headings":"Complete Example: Building a Custom Schema","what":"2. Create the Schema","title":"Configuring EcoExtract for Your Data","text":"","code":"# Save as ecoextract/schema.json {   \"$schema\": \"http://json-schema.org/draft-07/schema#\",   \"title\": \"Plant-Pollinator Interaction Schema\",   \"type\": \"object\",   \"additionalProperties\": false,   \"required\": [\"records\"],   \"properties\": {     \"records\": {       \"type\": \"array\",       \"items\": {         \"type\": \"object\",         \"additionalProperties\": false,         \"required\": [           \"plant_species\",           \"pollinator_species\",           \"interaction_type\",           \"location\",           \"season\",           \"all_supporting_source_sentences\"         ],         \"properties\": {           \"plant_species\": {             \"type\": \"string\",             \"description\": \"Scientific name of plant species\"           },           \"pollinator_species\": {             \"type\": \"string\",             \"description\": \"Scientific name of pollinator species\"           },           \"interaction_type\": {             \"type\": \"array\",             \"items\": {               \"type\": \"string\",               \"description\": \"Type of interaction: pollination, nectar_robbing, etc.\"             }           },           \"location\": {             \"type\": [\"string\", \"null\"],             \"description\": \"Geographic location of observation\"           },           \"season\": {             \"type\": [\"string\", \"null\"],             \"description\": \"Season or months when interaction observed\"           },           \"all_supporting_source_sentences\": {             \"type\": \"array\",             \"items\": {               \"type\": \"string\"             }           }         },         \"x-unique-fields\": [\"plant_species\", \"pollinator_species\", \"location\"]       }     }   } }"},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"write-the-prompt","dir":"Articles","previous_headings":"Complete Example: Building a Custom Schema","what":"3. Write the Prompt","title":"Configuring EcoExtract for Your Data","text":"","code":"# Save as ecoextract/extraction_prompt.md  # Task Extract all plant-pollinator interactions reported in this document.  # Field Definitions  **plant_species**: Scientific name of the plant (genus + species).  **pollinator_species**: Scientific name of the pollinator. Use the most specific taxonomic level available.  **interaction_type**: Type(s) of interaction. Common types: - pollination (legitimate pollination) - nectar_robbing (accessing nectar without pollinating) - pollen_collecting (collecting pollen but not pollinating)  **location**: Geographic location. Use most specific available (site > region > country).  **season**: When the interaction was observed (season, months, or dates).  **all_supporting_source_sentences**: All sentences providing information about this interaction.  # Extraction Rules  Create one record per unique plant-pollinator-location combination.  If the same pair is observed in multiple seasons at the same location, create separate records.  # Examples  \"Bombus terrestris was observed visiting Trifolium repens flowers in meadows near Oxford during June and July.\"  → One record: - plant_species: \"Trifolium repens\" - pollinator_species: \"Bombus terrestris\" - interaction_type: [\"pollination\"] - location: \"meadows near Oxford\" - season: \"June and July\""},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"process-and-test","dir":"Articles","previous_headings":"Complete Example: Building a Custom Schema","what":"4. Process and Test","title":"Configuring EcoExtract for Your Data","text":"","code":"# Process test documents process_documents(   pdf_path = \"pollinator_papers/\",   db_conn = \"pollinators.db\" )  # Review results records <- get_records(db_conn = \"pollinators.db\")  # Check extraction quality library(ecoreview) run_review_app(\"pollinators.db\")  # Calculate accuracy after review accuracy <- calculate_accuracy(\"pollinators.db\")"},{"path":[]},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"schema-validation-errors","dir":"Articles","previous_headings":"Troubleshooting","what":"Schema Validation Errors","title":"Configuring EcoExtract for Your Data","text":"","code":"# Validate JSON syntax jsonlite::validate(\"ecoextract/schema.json\")  # Read and inspect schema schema <- jsonlite::read_json(\"ecoextract/schema.json\") names(schema$properties)  # Should include \"records\"  # Test with default schema first process_documents(\"test.pdf\", \"test.db\", schema_file = NULL)"},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"llm-not-following-instructions","dir":"Articles","previous_headings":"Troubleshooting","what":"LLM Not Following Instructions","title":"Configuring EcoExtract for Your Data","text":"LLM isn’t extracting correctly: Make prompts explicit - Add examples edge cases Simplify field descriptions - Use clear, unambiguous language Check required fields - many required fields can cause failures Test incrementally - Start simple schema, add complexity gradually","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"array-fields-not-working","dir":"Articles","previous_headings":"Troubleshooting","what":"Array Fields Not Working","title":"Configuring EcoExtract for Your Data","text":"","code":"# Check if arrays are being recognized records <- get_records() class(records$your_array_field)  # Should be \"list\"  # If it's character, arrays aren't being parsed correctly # Check schema has proper array definition: # \"type\": \"array\", \"items\": {\"type\": \"string\"}"},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next Steps","title":"Configuring EcoExtract for Your Data","text":"Process documents custom schema Launch review app validate extraction quality Iterate configuration based results Calculate accuracy measure performance","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/configuration.html","id":"additional-resources","dir":"Articles","previous_headings":"Next Steps","what":"Additional Resources","title":"Configuring EcoExtract for Your Data","text":"Complete Workflow Guide - Full workflow reference ACCURACY.md - Understanding accuracy metrics SCHEMA_GUIDE.md - Detailed schema specification OpenAI Structured Outputs - Schema requirements cross-provider compatibility","code":""},{"path":[]},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"the-three-package-ecosystem","dir":"Articles","previous_headings":"Setup","what":"The Three-Package Ecosystem","title":"EcoExtract: Complete Workflow Guide","text":"EcoExtract works part three packages:","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"prerequisites","dir":"Articles","previous_headings":"Setup","what":"Prerequisites","title":"EcoExtract: Complete Workflow Guide","text":"R version 4.1.0 higher RStudio (recommended) API keys Tensorlake (OCR) Anthropic Claude (extraction)","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"installation","dir":"Articles","previous_headings":"Setup","what":"Installation","title":"EcoExtract: Complete Workflow Guide","text":"Install three packages GitHub: Optional dependencies: crew – parallel processing (install.packages(\"crew\")) ecoreview – human review extracted records","code":"# Using pak (recommended) pak::pak(\"n8layman/ohseer\")      # OCR processing pak::pak(\"n8layman/ecoextract\")  # Data extraction pak::pak(\"n8layman/ecoreview\")   # Review app (optional)  # Or using devtools devtools::install_github(\"n8layman/ohseer\") devtools::install_github(\"n8layman/ecoextract\") devtools::install_github(\"n8layman/ecoreview\") library(ecoextract)"},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"api-key-setup","dir":"Articles","previous_headings":"Setup","what":"API Key Setup","title":"EcoExtract: Complete Workflow Guide","text":"EcoExtract requires API keys OCR data extraction. Required API keys: Tensorlake (OCR, default): https://www.tensorlake.ai/ Anthropic Claude (extraction): https://console.anthropic.com/ Optional OCR providers (via ohseer): Mistral: https://console.mistral.ai/ Claude: https://console.anthropic.com/ (uses key extraction) creating .env file, verify ’s .gitignore: Create .env file project root: .env file automatically loaded R starts project directory. can also load manually: Verify committing:","code":"# Check that .env is in .gitignore grep \"^\\.env$\" .gitignore  # If not found, add it NOW before creating the file: echo \".env\" >> .gitignore ANTHROPIC_API_KEY=sk-ant-api03-your-key-here TENSORLAKE_API_KEY=your-tensorlake-key-here # Optional for alternative OCR providers MISTRAL_API_KEY=your-mistral-key-here readRenviron(\".env\")  # Or set keys directly in R Sys.setenv(ANTHROPIC_API_KEY = \"your_key_here\") Sys.setenv(TENSORLAKE_API_KEY = \"your_key_here\")  # Verify keys are loaded Sys.getenv(\"ANTHROPIC_API_KEY\") Sys.getenv(\"TENSORLAKE_API_KEY\") # This should show no output if .env is properly ignored: git status | grep \".env\""},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"verify-installation","dir":"Articles","previous_headings":"Setup","what":"Verify Installation","title":"EcoExtract: Complete Workflow Guide","text":"","code":"library(ecoextract)  # Check that functions are available ?process_documents ?get_records"},{"path":[]},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"the-four-step-pipeline","dir":"Articles","previous_headings":"Processing Documents","what":"The Four-Step Pipeline","title":"EcoExtract: Complete Workflow Guide","text":"process_documents() orchestrates four-step extraction pipeline: OCR Processing (via ohseer) – Convert PDF markdown text embedded images Metadata Extraction – Extract publication metadata (title, authors, DOI, etc.) Data Extraction – Extract structured records using Claude according schema Refinement (optional) – Enhance verify extracted data","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"quick-start","dir":"Articles","previous_headings":"Processing Documents","what":"Quick Start","title":"EcoExtract: Complete Workflow Guide","text":"automatically handles OCR, metadata extraction, data extraction Claude, saving SQLite, smart skip logic (re-running skips completed steps).","code":"# Process a single PDF results <- process_documents(   pdf_path = \"my_paper.pdf\",   db_conn = \"ecoextract_records.db\" )  # Process all PDFs in a folder results <- process_documents(   pdf_path = \"pdfs/\",   db_conn = \"ecoextract_records.db\" )  # Process with refinement enabled results <- process_documents(   pdf_path = \"pdfs/\",   db_conn = \"ecoextract_records.db\",   run_refinement = TRUE )"},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"parallel-processing","dir":"Articles","previous_headings":"Processing Documents","what":"Parallel Processing","title":"EcoExtract: Complete Workflow Guide","text":"faster processing multiple documents, use parallel processing crew package: Benefits: worker processes complete document (4 steps) Crash-resilient: completed documents saved immediately Progress shown documents complete: [1/10] paper.pdf completed Re-run resume: skip logic detects completed documents","code":"install.packages(\"crew\")  # Process with 4 parallel workers results <- process_documents(   pdf_path = \"pdfs/\",   db_conn = \"ecoextract_records.db\",   workers = 4,   log = TRUE  # Creates ecoextract_YYYYMMDD_HHMMSS.log )"},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"model-fallback","dir":"Articles","previous_headings":"Processing Documents","what":"Model Fallback","title":"EcoExtract: Complete Workflow Guide","text":"EcoExtract supports tiered model fallback handle content refusals (e.g., Claude refusing disease/biosecurity papers). Provide vector models try sequentially: Audit logging: database tracks model succeeded step (metadata, extraction, refinement) *_llm_model columns. failed attempts error messages timestamps logged *_log columns debugging. API keys: Add keys fallback providers .env:","code":"# Single model (default) process_documents(   pdf_path = \"papers/\",   model = \"anthropic/claude-sonnet-4-5\" )  # Tiered fallback: try Claude, then GPT-4o, then Mistral process_documents(   pdf_path = \"papers/\",   model = c(     \"anthropic/claude-sonnet-4-5\",     \"openai/gpt-4o\",     \"mistral/mistral-large-latest\"   ) ) ANTHROPIC_API_KEY=your_anthropic_key OPENAI_API_KEY=your_openai_key MISTRAL_API_KEY=your_mistral_key"},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"ocr-provider-selection","dir":"Articles","previous_headings":"Processing Documents","what":"OCR Provider Selection","title":"EcoExtract: Complete Workflow Guide","text":"EcoExtract supports multiple OCR providers ohseer. default uses Tensorlake, can switch Mistral Claude, use provider fallback: Provider tracking: OCR provider succeeded document tracked ocr_provider column documents table. using provider fallback, ohseer automatically tries provider order records one succeeded. API keys: OCR providers require API keys .env:","code":"# Use default provider (Tensorlake) process_documents(\"papers/\")  # Use Mistral OCR (better structure preservation) process_documents(   pdf_path = \"papers/\",   ocr_provider = \"mistral\" )  # Use Claude OCR process_documents(   pdf_path = \"papers/\",   ocr_provider = \"claude\" )  # OCR fallback: try Mistral, then Tensorlake if Mistral fails process_documents(   pdf_path = \"papers/\",   ocr_provider = c(\"mistral\", \"tensorlake\") )  # Increase OCR timeout for large documents process_documents(   pdf_path = \"papers/\",   ocr_timeout = 300  # 5 minutes ) TENSORLAKE_API_KEY=your_tensorlake_key  # Required for default MISTRAL_API_KEY=your_mistral_key        # Optional ANTHROPIC_API_KEY=your_anthropic_key    # For both extraction and Claude OCR"},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"skip-logic","dir":"Articles","previous_headings":"Processing Documents","what":"Skip Logic","title":"EcoExtract: Complete Workflow Guide","text":"re-run process_documents(), automatically skips steps already completed. allows resume interrupted processing, add new PDFs, re-run specific steps fixing issues.","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"skip-behavior","dir":"Articles","previous_headings":"Processing Documents > Skip Logic","what":"Skip Behavior","title":"EcoExtract: Complete Workflow Guide","text":"step checks status database:","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"cascade-logic","dir":"Articles","previous_headings":"Processing Documents > Skip Logic","what":"Cascade Logic","title":"EcoExtract: Complete Workflow Guide","text":"step forced re-run, downstream steps automatically invalidated:","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"force-reprocessing","dir":"Articles","previous_headings":"Processing Documents > Skip Logic","what":"Force Reprocessing","title":"EcoExtract: Complete Workflow Guide","text":"Override skip logic force reprocessing: force_reprocess_* parameter accepts: NULL (default) – use normal skip logic TRUE – force reprocess documents Integer vector (e.g., c(5L, 12L)) – force reprocess specific document IDs","code":"# Force reprocess all documents from OCR onward results <- process_documents(   pdf_path = \"pdfs/\",   db_conn = \"ecoextract_records.db\",   force_reprocess_ocr = TRUE )  # Force reprocess specific documents only (by document_id) results <- process_documents(   pdf_path = \"pdfs/\",   db_conn = \"ecoextract_records.db\",   force_reprocess_extraction = c(5L, 12L) )"},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"deduplication","dir":"Articles","previous_headings":"Processing Documents","what":"Deduplication","title":"EcoExtract: Complete Workflow Guide","text":"extraction, records automatically deduplicated existing records database. Three similarity methods available: Methods: \"llm\" (default) – Uses Claude semantically compare records \"embedding\" – Cosine similarity text embeddings \"jaccard\" – Fast n-gram based comparison (API calls)","code":"# Default: LLM-based deduplication (most accurate) results <- process_documents(\"pdfs/\", db_conn = \"records.db\",                             similarity_method = \"llm\")  # Embedding-based with custom threshold results <- process_documents(\"pdfs/\", db_conn = \"records.db\",                             similarity_method = \"embedding\",                             min_similarity = 0.85)  # Fast local deduplication (no API calls) results <- process_documents(\"pdfs/\", db_conn = \"records.db\",                             similarity_method = \"jaccard\",                             min_similarity = 0.9)"},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"customizing-for-your-domain","dir":"Articles","previous_headings":"","what":"Customizing for Your Domain","title":"EcoExtract: Complete Workflow Guide","text":"EcoExtract domain-agnostic. Customize editing schema extraction prompt.","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"initialize-custom-configuration","dir":"Articles","previous_headings":"Customizing for Your Domain","what":"Initialize Custom Configuration","title":"EcoExtract: Complete Workflow Guide","text":"Edit files ecoextract/ research domain, process usual – package automatically detects configuration files directory: detailed guidance writing schemas prompts, see Configuration Guide.","code":"# Creates ecoextract/ directory with template files init_ecoextract()  # This creates: # - ecoextract/SCHEMA_GUIDE.md      # Read this first! # - ecoextract/schema.json          # Edit for your domain # - ecoextract/extraction_prompt.md # Edit for your domain # Automatically uses ecoextract/schema.json and ecoextract/extraction_prompt.md process_documents(\"pdfs/\", \"ecoextract_records.db\")  # Or specify custom files explicitly process_documents(   pdf_path = \"pdfs/\",   db_conn = \"ecoextract_records.db\",   schema_file = \"my_custom/schema.json\",   extraction_prompt_file = \"my_custom/extraction.md\" )"},{"path":[]},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"query-records","dir":"Articles","previous_headings":"Retrieving Your Data","what":"Query Records","title":"EcoExtract: Complete Workflow Guide","text":"","code":"# Get all records from all documents all_records <- get_records()  # Get records from a specific document doc_records <- get_records(document_id = 1)  # Use a custom database path records <- get_records(db_conn = \"my_project.db\")"},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"query-documents","dir":"Articles","previous_headings":"Retrieving Your Data","what":"Query Documents","title":"EcoExtract: Complete Workflow Guide","text":"","code":"# Get all documents and their metadata all_docs <- get_documents()  # Check processing status all_docs$ocr_status          # \"completed\", \"pending\", \"failed\" all_docs$metadata_status all_docs$extraction_status"},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"export-data","dir":"Articles","previous_headings":"Retrieving Your Data","what":"Export Data","title":"EcoExtract: Complete Workflow Guide","text":"export_db() function joins records document metadata: exported data includes document metadata (title, authors, journal, DOI), extracted record fields (defined schema), processing status timestamps.","code":"# Get all records with metadata as a tibble data <- export_db()  # Export to CSV file export_db(filename = \"extracted_data.csv\")  # Export only records from specific document export_db(document_id = 1, filename = \"document_1.csv\")  # Include OCR content in export (large files!) data <- export_db(include_ocr = TRUE)  # Simplified export (removes processing metadata columns) data <- export_db(simple = TRUE)"},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"view-ocr-results","dir":"Articles","previous_headings":"Retrieving Your Data","what":"View OCR Results","title":"EcoExtract: Complete Workflow Guide","text":"","code":"# Get OCR markdown text markdown <- get_ocr_markdown(document_id = 1) cat(markdown)  # View OCR with embedded images in RStudio Viewer get_ocr_html_preview(document_id = 1)  # View all pages get_ocr_html_preview(document_id = 1, page_num = \"all\")"},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"human-review-workflow","dir":"Articles","previous_headings":"","what":"Human Review Workflow","title":"EcoExtract: Complete Workflow Guide","text":"extraction, review correct results using ecoreview Shiny app.","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"launch-the-review-app","dir":"Articles","previous_headings":"Human Review Workflow","what":"Launch the Review App","title":"EcoExtract: Complete Workflow Guide","text":"app provides: Document--document review – Navigate processed documents Side--side view – See OCR text extracted records together Edit records – Modify extracted data directly Add records – Manually add records LLM missed Delete records – Remove incorrect records Automatic audit trail – edits tracked record_edits table","code":"library(ecoreview) run_review_app(db_path = \"ecoextract_records.db\")"},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"review-workflow","dir":"Articles","previous_headings":"Human Review Workflow","what":"Review Workflow","title":"EcoExtract: Complete Workflow Guide","text":"Process documents EcoExtract Launch review app database Review document – check records source text, edit, add, delete needed, click “Accept” Export final data corrections","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"edit-tracking","dir":"Articles","previous_headings":"Human Review Workflow","what":"Edit Tracking","title":"EcoExtract: Complete Workflow Guide","text":"save_document() function (used ecoreview) tracks changes: Column-level edits – Knows exactly fields changed Original values – Stores LLM’s original extraction Edit timestamps – change made Added/deleted flags – Distinguishes human-added vs LLM-extracted records audit trail enables accuracy calculations, understanding LLM performance, quality control. information, see ecoreview repository.","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"calculate-accuracy-metrics","dir":"Articles","previous_headings":"Human Review Workflow","what":"Calculate Accuracy Metrics","title":"EcoExtract: Complete Workflow Guide","text":"reviewing documents, calculate comprehensive accuracy metrics: EcoExtract provides nuanced accuracy metrics separate: Record detection – Finding records vs missing/hallucinating Field-level accuracy – Correctness individual fields (gives partial credit) Edit severity – Major edits (unique/required fields) vs minor edits complete explanation, see ACCURACY.md. Accuracy visualizations available ecoreview Shiny app.","code":"accuracy <- calculate_accuracy(\"ecoextract_records.db\")  # View key metrics accuracy$detection_recall      # Did we find the records? accuracy$field_precision       # How accurate were the fields? accuracy$field_f1              # Overall field-level F1 score accuracy$major_edit_rate       # How serious were the errors? accuracy$avg_edits_per_document # Average corrections needed"},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"complete-example","dir":"Articles","previous_headings":"","what":"Complete Example","title":"EcoExtract: Complete Workflow Guide","text":"End--end workflow processing review export:","code":"library(ecoextract)  # 1. Initialize custom configuration (first time only) init_ecoextract() # Edit ecoextract/schema.json and ecoextract/extraction_prompt.md  # 2. Process documents with parallel processing results <- process_documents(   pdf_path = \"papers/\",   db_conn = \"records.db\",   workers = 4,   log = TRUE )  # 3. Launch review app library(ecoreview) run_review_app(db_path = \"records.db\") # Review and edit records in the Shiny app  # 4. Export final data final_data <- export_db(   db_conn = \"records.db\",   filename = \"final_data.csv\" )  # 5. Check results library(dplyr) edited_records <- final_data |> filter(human_edited == TRUE) cat(\"Total records:\", nrow(final_data), \"\\n\") cat(\"Edited records:\", nrow(edited_records), \"\\n\")  # 6. Calculate accuracy accuracy <- calculate_accuracy(\"records.db\")"},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"database-schema","dir":"Articles","previous_headings":"","what":"Database Schema","title":"EcoExtract: Complete Workflow Guide","text":"SQLite database two main tables: documents – Stores document metadata processing status: document_id, file_name, file_path – Identity title, authors, publication_year, journal, doi – Publication metadata document_content – OCR markdown text ocr_status, metadata_status, extraction_status, refinement_status – Processing status workflow step ocr_provider – OCR provider succeeded (tensorlake, mistral, claude) metadata_llm_model, extraction_llm_model, refinement_llm_model – model succeeded LLM step ocr_log, metadata_log, extraction_log, refinement_log – Audit trail failed attempts error messages timestamps (JSON) records_extracted – Count records extracted records – Stores extracted data records: id – Primary key (auto-increment) document_id – Foreign key documents record_id – Human-readable identifier (e.g., “Smith2023-001”) Custom fields defined schema extraction_timestamp, prompt_hash – Metadata record_edits – Audit trail human edits: Tracks column-level changes original values timestamps","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"best-practices","dir":"Articles","previous_headings":"","what":"Best Practices","title":"EcoExtract: Complete Workflow Guide","text":"Start small. Test 2-3 papers first. Review results processing entire corpus. Use parallel processing large batches. 10+ papers, workers = 4 significantly speeds processing. Enable refinement selectively. Run refinement documents need : run_refinement = c(5L, 12L, 18L). Review early often. Process small batch, review immediately ecoreview, iterate schema prompts processing . Version control configs. Add ecoextract/schema.json ecoextract/extraction_prompt.md git. Keep .env (API keys) .gitignore. Monitor API usage. Track usage https://console.anthropic.com/. Typical per-paper usage: OCR ~2-5K tokens, metadata ~1-2K, extraction ~5-10K, refinement ~3-5K.","code":""},{"path":[]},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"api-key-not-found","dir":"Articles","previous_headings":"Troubleshooting","what":"API Key Not Found","title":"EcoExtract: Complete Workflow Guide","text":"","code":"# Reload from .env file readRenviron(\".env\")  # Or set directly Sys.setenv(ANTHROPIC_API_KEY = \"sk-ant-...\")  # Verify Sys.getenv(\"ANTHROPIC_API_KEY\")"},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"database-locked-errors","dir":"Articles","previous_headings":"Troubleshooting","what":"Database Locked Errors","title":"EcoExtract: Complete Workflow Guide","text":"get “database locked” parallel processing, usually resolves automatically. Verify WAL mode enabled:","code":"library(DBI) db <- dbConnect(RSQLite::SQLite(), \"records.db\") dbGetQuery(db, \"PRAGMA journal_mode\")  # Should return \"wal\" dbGetQuery(db, \"PRAGMA busy_timeout\")  # Should return 30000 dbDisconnect(db)"},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"schema-validation-errors","dir":"Articles","previous_headings":"Troubleshooting","what":"Schema Validation Errors","title":"EcoExtract: Complete Workflow Guide","text":"","code":"# Validate JSON syntax jsonlite::validate(\"ecoextract/schema.json\")  # Verify structure (must have top-level \"records\" property) schema <- jsonlite::read_json(\"ecoextract/schema.json\") names(schema$properties)  # Should include \"records\"  # Test with default schema first process_documents(\"test.pdf\", \"test.db\", schema_file = NULL)"},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"ocr-failures","dir":"Articles","previous_headings":"Troubleshooting","what":"OCR Failures","title":"EcoExtract: Complete Workflow Guide","text":"","code":"# Check which documents failed docs <- get_documents() failed <- docs |> dplyr::filter(ocr_status == \"failed\")  # Force reprocess failed OCR process_documents(   pdf_path = \"pdfs/\",   db_conn = \"records.db\",   force_reprocess_ocr = failed$document_id )"},{"path":"https://n8layman.github.io/ecoextract/articles/ecoextract-workflow.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next Steps","title":"EcoExtract: Complete Workflow Guide","text":"Configuration Guide – Customize schema prompts domain Testing Guide – Running writing tests ACCURACY.md – Understanding accuracy metrics depth Function docs: ?process_documents, ?get_records, ?export_db, ?calculate_accuracy Repos: ecoextract | ecoreview | ohseer","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/testing.html","id":"core-principle","dir":"Articles","previous_headings":"","what":"Core Principle","title":"Testing Philosophy","text":"Test functionality, domain content. EcoExtract tests verify package’s core operations work correctly regardless specific schema, prompts, ecological domain. Tests never check whether LLM produces “correct” extractions – verify pipeline machinery functions properly. means: schema-specific assertions – tests don’t look field names like bat_species pathogen_name prompt effectiveness testing – tests don’t check whether extraction finds specific data text LLM quality evaluation – tests verify API calls return valid structure, accurate content Schema-agnostic fixtures – test schemas deliberately differ package defaults","code":""},{"path":[]},{"path":"https://n8layman.github.io/ecoextract/articles/testing.html","id":"local-tests-no-api-keys-required","dir":"Articles","previous_headings":"Test Categories","what":"Local Tests (no API keys required)","title":"Testing Philosophy","text":"tests run entirely offline execute second. always run devtools::test() devtools::check(). test-database.R – Database operations: Database initialization creates required tables (documents, records, record_edits) Database schema matches JSON schema definition Records can saved retrieved Array fields stored single-level JSON arrays test-review.R – Human review accuracy: save_document() updates reviewed_at timestamp Modified records marked human_edited Deleted records marked deleted_by_user Edit tracking populates record_edits table calculate_accuracy() returns correct structure metrics test-utils.R – Utility functions: Record ID generation formatting Special character handling IDs Token estimation various inputs test-deduplication.R – Deduplication logic: Text canonicalization (Unicode normalization, case folding, whitespace trimming) Cosine similarity Jaccard similarity calculation Jaccard-based deduplication (exact duplicates, typos, partial matches) Schema validation (x-unique-fields required valid) test-bibtex.R – BibTeX export: Document metadata exports valid BibTeX entries Citation extraction bibliography field Handles incomplete metadata gracefully","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/testing.html","id":"integration-tests-require-api-keys","dir":"Articles","previous_headings":"Test Categories","what":"Integration Tests (require API keys)","title":"Testing Philosophy","text":"tests make real API calls validate end--end pipeline. automatically skipped required API keys set, contributors without keys can still run local test suite. Required API keys: test-integration.R – API-requiring tests one file: Full pipeline: PDF database (OCR, metadata, extraction, refinement) API failures captured status columns, thrown errors Schema-agnostic pipeline host-pathogen schema (proves hard-coded assumptions) Embedding-based deduplication (exact duplicates, near-duplicates, missing fields) Field--field deduplication (partial matches, populated field comparison) LLM-based semantic deduplication (common names vs scientific names)","code":""},{"path":"https://n8layman.github.io/ecoextract/articles/testing.html","id":"running-tests","dir":"Articles","previous_headings":"","what":"Running Tests","title":"Testing Philosophy","text":"include integration tests, set API keys .env file (see Complete Guide details). .env file automatically loaded R starts project directory.","code":"# Run all tests (integration tests auto-skip without keys) devtools::test()  # Run a specific test file testthat::test_file(\"tests/testthat/test-database.R\")  # Full package check (includes tests, documentation, examples) devtools::check()"},{"path":"https://n8layman.github.io/ecoextract/articles/testing.html","id":"design-patterns","dir":"Articles","previous_headings":"","what":"Design Patterns","title":"Testing Philosophy","text":"Cleanup withr. test resources (temp databases, files, environment variables) cleaned automatically using withr, ensuring side effects tests. Focused assertions. test_that() block tests one specific behavior rather bundling multiple concerns. Edge case coverage. Tests cover typical usage, edge cases (empty inputs, NULL values), error conditions, type validation. Schema-agnostic design. Test fixtures use schemas deliberately differ package defaults, proving hard-coded domain assumptions exist pipeline. API key gating. Integration tests use skip_if() guards full local test suite passes without API keys configured.","code":""},{"path":"https://n8layman.github.io/ecoextract/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Nathan C Layman. Author, maintainer.","code":""},{"path":"https://n8layman.github.io/ecoextract/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Layman N (2026). ecoextract: Ecological Data Extraction Refinement. R package version 0.1.2, https://github.com/n8layman/ecoextract.","code":"@Manual{,   title = {ecoextract: Ecological Data Extraction and Refinement},   author = {Nathan C Layman},   year = {2026},   note = {R package version 0.1.2},   url = {https://github.com/n8layman/ecoextract}, }"},{"path":"https://n8layman.github.io/ecoextract/index.html","id":"ecoextract","dir":"","previous_headings":"","what":"Ecological Data Extraction and Refinement","title":"Ecological Data Extraction and Refinement","text":"Structured ecological data extraction refinement scientific literature. EcoExtract automates extraction structured data PDFs using OCR LLMs. ’s domain-agnostic works JSON schema define.","code":""},{"path":"https://n8layman.github.io/ecoextract/index.html","id":"pipeline","dir":"","previous_headings":"","what":"Pipeline","title":"Ecological Data Extraction and Refinement","text":"Quick Links: Complete Guide (Installation, workflow, review, accuracy) Configuration Guide (Custom schemas prompts) Accuracy Metrics Guide (Understanding accuracy calculations)","code":"graph LR     A[PDF Papers] -->|ohseer| B[OCR]     B -->|ecoextract| C[Metadata]     B -->|ecoextract + Claude| D[Data Extraction]     C --> E[SQLite Database]     D --> E     E -.->|optional| F[Refinement]     F -.-> E     E -->|ecoreview| G[Human Review]     G --> H[Validated Data]      style A fill:#e1f5ff     style H fill:#c8e6c9     style G fill:#fff9c4     style E fill:#f0f0f0"},{"path":"https://n8layman.github.io/ecoextract/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Ecological Data Extraction and Refinement","text":"See Complete Guide alternative installation methods troubleshooting.","code":"# Install the ecosystem (pak recommended) pak::pak(\"n8layman/ohseer\")      # OCR processing pak::pak(\"n8layman/ecoextract\")  # Data extraction pak::pak(\"n8layman/ecoreview\")   # Review app (optional)"},{"path":"https://n8layman.github.io/ecoextract/index.html","id":"api-key-setup","dir":"","previous_headings":"","what":"API Key Setup","title":"Ecological Data Extraction and Refinement","text":"EcoExtract uses ellmer LLM interactions ohseer OCR. Required API keys: Tensorlake (OCR, default): https://www.tensorlake.ai/ Anthropic Claude (extraction): https://console.anthropic.com/ Optional OCR providers (via ohseer): Mistral: https://console.mistral.ai/ Claude: https://console.anthropic.com/ (uses key extraction) Create .env file project root (make sure ’s .gitignore first!): .env file automatically loaded R starts project directory. See Complete Guide detailed setup instructions. default, ecoextract uses anthropic/claude-sonnet-4-5 extraction tensorlake OCR. use different providers, pass model ocr_provider parameters process_documents(). Note: non-Anthropic LLM providers fully tested.","code":"ANTHROPIC_API_KEY=your_anthropic_api_key_here TENSORLAKE_API_KEY=your_tensorlake_api_key_here # Optional for alternative OCR providers MISTRAL_API_KEY=your_mistral_api_key_here"},{"path":"https://n8layman.github.io/ecoextract/index.html","id":"quick-start","dir":"","previous_headings":"","what":"Quick Start","title":"Ecological Data Extraction and Refinement","text":"","code":"library(ecoextract)  # Process all PDFs in a folder through the 4-step pipeline: # OCR -> Metadata -> Extraction -> Refinement (optional) results <- process_documents(   pdf_path = \"path/to/pdfs/\",   db_conn = \"ecoextract_records.db\" )  # Retrieve your data records <- get_records() export_db(filename = \"extracted_data.csv\")"},{"path":"https://n8layman.github.io/ecoextract/index.html","id":"model-fallback","dir":"","previous_headings":"","what":"Model Fallback","title":"Ecological Data Extraction and Refinement","text":"EcoExtract supports tiered model fallback handle content refusals (e.g., Claude refusing disease/biosecurity papers). Provide vector models try sequentially: Audit logging: database tracks model succeeded step (metadata, extraction, refinement) *_llm_model columns. failed attempts error messages timestamps logged *_log columns debugging. API keys: Add keys fallback providers .env:","code":"# Single model (default) process_documents(   pdf_path = \"papers/\",   model = \"anthropic/claude-sonnet-4-5\" )  # Tiered fallback: try Claude, then GPT-4o, then Mistral process_documents(   pdf_path = \"papers/\",   model = c(     \"anthropic/claude-sonnet-4-5\",     \"openai/gpt-4o\",     \"mistral/mistral-large-latest\"   ) ) ANTHROPIC_API_KEY=your_anthropic_key OPENAI_API_KEY=your_openai_key MISTRAL_API_KEY=your_mistral_key"},{"path":"https://n8layman.github.io/ecoextract/index.html","id":"ocr-provider-selection","dir":"","previous_headings":"","what":"OCR Provider Selection","title":"Ecological Data Extraction and Refinement","text":"EcoExtract supports multiple OCR providers ohseer. default uses Tensorlake, can switch Mistral Claude: Provider fallback: multiple providers specified, ohseer automatically tries order one succeeds. OCR provider used document tracked ocr_provider column documents table.","code":"# Use default provider (Tensorlake) process_documents(\"papers/\")  # Use Mistral OCR process_documents(   pdf_path = \"papers/\",   ocr_provider = \"mistral\" )  # Use Claude OCR process_documents(   pdf_path = \"papers/\",   ocr_provider = \"claude\" )  # OCR fallback: try Mistral, then Tensorlake if Mistral fails process_documents(   pdf_path = \"papers/\",   ocr_provider = c(\"mistral\", \"tensorlake\") )  # Increase OCR timeout for large documents process_documents(   pdf_path = \"papers/\",   ocr_timeout = 300  # 5 minutes )"},{"path":"https://n8layman.github.io/ecoextract/index.html","id":"key-features","dir":"","previous_headings":"","what":"Key Features","title":"Ecological Data Extraction and Refinement","text":"Smart skip logic – Re-running process_documents() skips completed steps. Forced re-runs automatically invalidate downstream steps. Parallel processing – Process multiple documents simultaneously workers = 4 (requires crew package). Deduplication – Three methods: \"llm\" (default), \"embedding\", \"jaccard\". Human review – Edit, add, delete records ecoreview Shiny app full audit trail. Accuracy metrics – Calculate detection recall, field precision, F1, edit severity review. See Complete Guide details features.","code":""},{"path":"https://n8layman.github.io/ecoextract/index.html","id":"custom-schemas","dir":"","previous_headings":"","what":"Custom Schemas","title":"Ecological Data Extraction and Refinement","text":"EcoExtract domain-agnostic works JSON schema: Schema requirements: top-level records property (array objects), field type description, JSON Schema draft-07 format. See Configuration Guide complete details examples.","code":"# Create custom config directory with templates init_ecoextract()  # Edit the generated files: # - ecoextract/schema.json          # Define your data structure # - ecoextract/extraction_prompt.md # Describe what to extract  # The package automatically uses these files process_documents(\"pdfs/\", \"records.db\")"},{"path":[]},{"path":"https://n8layman.github.io/ecoextract/index.html","id":"workflow","dir":"","previous_headings":"Package Functions","what":"Workflow","title":"Ecological Data Extraction and Refinement","text":"process_documents() - Complete 4-step workflow (OCR -> Metadata -> Extract -> Refine)","code":""},{"path":"https://n8layman.github.io/ecoextract/index.html","id":"database-setup","dir":"","previous_headings":"Package Functions","what":"Database Setup","title":"Ecological Data Extraction and Refinement","text":"init_ecoextract_database() - Initialize database schema init_ecoextract() - Create project config directory template schema prompts","code":""},{"path":"https://n8layman.github.io/ecoextract/index.html","id":"data-access","dir":"","previous_headings":"Package Functions","what":"Data Access","title":"Ecological Data Extraction and Refinement","text":"get_documents() - Query documents metadata database get_records() - Query extracted records database get_ocr_markdown() - Get OCR markdown text document get_ocr_html_preview() - Render OCR output embedded images HTML get_db_stats() - Get document record counts database export_db() - Export records metadata tibble CSV file","code":""},{"path":"https://n8layman.github.io/ecoextract/index.html","id":"testing","dir":"","previous_headings":"","what":"Testing","title":"Ecological Data Extraction and Refinement","text":"Integration tests require API keys .env file. See CONTRIBUTING.md details.","code":"devtools::test()   # Run all tests devtools::check()  # Run package checks"},{"path":"https://n8layman.github.io/ecoextract/index.html","id":"file-structure","dir":"","previous_headings":"","what":"File Structure","title":"Ecological Data Extraction and Refinement","text":"","code":"ecoextract/ ├── R/ │   ├── workflow.R          # Main process_documents() workflow + skip/cascade logic │   ├── ocr.R               # OCR processing │   ├── metadata.R          # Publication metadata extraction │   ├── extraction.R        # Data extraction functions │   ├── refinement.R        # Data refinement functions │   ├── deduplication.R     # Record deduplication (LLM, embedding, Jaccard) │   ├── database.R          # Database operations │   ├── getters.R           # Data access functions (get_*, export_db) │   ├── config_loader.R     # Configuration file loading + init_ecoextract() │   ├── prompts.R           # Prompt loading │   ├── utils.R             # Utilities │   ├── config.R            # Package configuration │   └── ecoextract-package.R # Package metadata ├── inst/ │   ├── extdata/            # Schema files │   │   ├── schema.json │   │   └── metadata_schema.json │   └── prompts/            # System prompts │       ├── extraction_prompt.md │       ├── extraction_context.md │       ├── metadata_prompt.md │       ├── metadata_context.md │       ├── refinement_prompt.md │       ├── refinement_context.md │       └── deduplication_prompt.md ├── tests/testthat/         # Tests ├── vignettes/              # Package vignettes ├── DESCRIPTION ├── NAMESPACE ├── CONTRIBUTING.md         # Development guide └── README.md"},{"path":[]},{"path":"https://n8layman.github.io/ecoextract/index.html","id":"r-packages","dir":"","previous_headings":"Tech Stack","what":"R Packages","title":"Ecological Data Extraction and Refinement","text":"ellmer - Structured LLM outputs ohseer - OCR processing dplyr - Data manipulation DBI & RSQLite - Database operations jsonlite - JSON handling glue - String interpolation stringr & stringi - String manipulation digest - Hashing tidyllm - LLM deduplication","code":""},{"path":"https://n8layman.github.io/ecoextract/index.html","id":"external-apis","dir":"","previous_headings":"Tech Stack","what":"External APIs","title":"Ecological Data Extraction and Refinement","text":"Tensorlake - OCR processing (via ohseer) Anthropic Claude / OpenAI / LLM providers - Data extraction refinement (via ellmer)","code":""},{"path":"https://n8layman.github.io/ecoextract/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Ecological Data Extraction and Refinement","text":"GPL (>= 3)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/add_record_ids.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal utility functions Create record IDs for a batch of records (internal) — add_record_ids","title":"Internal utility functions Create record IDs for a batch of records (internal) — add_record_ids","text":"Internal utility functions Create record IDs batch records (internal)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/add_record_ids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal utility functions Create record IDs for a batch of records (internal) — add_record_ids","text":"","code":"add_record_ids(interactions, author_lastname, publication_year)"},{"path":"https://n8layman.github.io/ecoextract/reference/add_record_ids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal utility functions Create record IDs for a batch of records (internal) — add_record_ids","text":"interactions Dataframe records author_lastname Author lastname ID generation publication_year Publication year ID generation","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/add_record_ids.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal utility functions Create record IDs for a batch of records (internal) — add_record_ids","text":"Dataframe record_id column added","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/build_existing_records_context.html","id":null,"dir":"Reference","previous_headings":"","what":"Build existing records context for LLM prompts — build_existing_records_context","title":"Build existing records context for LLM prompts — build_existing_records_context","text":"Build existing records context LLM prompts","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/build_existing_records_context.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build existing records context for LLM prompts — build_existing_records_context","text":"","code":"build_existing_records_context(   existing_records,   document_id = NULL,   include_record_id = FALSE )"},{"path":"https://n8layman.github.io/ecoextract/reference/build_existing_records_context.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build existing records context for LLM prompts — build_existing_records_context","text":"existing_records Dataframe existing records document_id Optional document ID context include_record_id Whether include record_id (TRUE refinement, FALSE extraction)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/build_existing_records_context.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build existing records context for LLM prompts — build_existing_records_context","text":"Character string existing records formatted LLM context","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/calculate_accuracy.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate extraction accuracy metrics from verified documents — calculate_accuracy","title":"Calculate extraction accuracy metrics from verified documents — calculate_accuracy","text":"Computes field-level accuracy record detection metrics human-reviewed documents. includes records documents reviewed (reviewed_at NULL).","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/calculate_accuracy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate extraction accuracy metrics from verified documents — calculate_accuracy","text":"","code":"calculate_accuracy(db_conn, document_ids = NULL, schema_file = NULL)"},{"path":"https://n8layman.github.io/ecoextract/reference/calculate_accuracy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate extraction accuracy metrics from verified documents — calculate_accuracy","text":"db_conn Database connection path database file document_ids Optional vector document IDs include (default: verified) schema_file Optional path schema JSON file (uses default NULL)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/calculate_accuracy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate extraction accuracy metrics from verified documents — calculate_accuracy","text":"List accuracy metrics: Raw counts:   - verified_documents: count reviewed documents   - verified_records: total records verified documents   - model_extracted: records extracted model   - human_added: records added humans (model missed)   - deleted: records deleted humans (hallucinated)   - records_with_edits: records least one field edit   - column_edits: named vector edit counts per column Field-level metrics (accuracy individual fields):   - total_fields: total fields model extracted   - correct_fields: fields correct   - field_precision: correct_fields / total_fields   - field_recall: correct_fields / all_true_fields   - field_f1: harmonic mean field precision recall Record detection metrics (finding records):   - records_found: records model found (includes imperfect extractions)   - records_missed: records model failed find   - records_hallucinated: records model made   - detection_precision: records_found / model_extracted   - detection_recall: records_found / total_true_records   - perfect_record_rate: found records, many zero errors Per-column accuracy:   - column_accuracy: named vector per-column accuracy (1 - edits/model_extracted) Edit severity (based unique/required fields schema):   - major_edits: count edits unique required fields   - minor_edits: count edits fields   - major_edit_rate: major_edits / total_edits   - avg_edits_per_document: mean edits per verified document","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/calculate_accuracy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate extraction accuracy metrics from verified documents — calculate_accuracy","text":"Separates two questions: 1. Record detection: model find record vs hallucinate vs miss ? 2. Field accuracy: fields extracted, many correct? Edit severity classified based schema: - Major edits: Changes unique fields (x-unique-fields) required fields - Minor edits: Changes optional descriptive fields","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/calculate_fields_changed.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate number of fields changed between original and refined records — calculate_fields_changed","title":"Calculate number of fields changed between original and refined records — calculate_fields_changed","text":"Compares schema fields (excluding metadata) count many changed refinement.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/calculate_fields_changed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate number of fields changed between original and refined records — calculate_fields_changed","text":"","code":"calculate_fields_changed(original_record, refined_record)"},{"path":"https://n8layman.github.io/ecoextract/reference/calculate_fields_changed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate number of fields changed between original and refined records — calculate_fields_changed","text":"original_record Single row dataframe named list original record refined_record Single row dataframe named list refined record","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/calculate_fields_changed.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate number of fields changed between original and refined records — calculate_fields_changed","text":"Integer count fields changed","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/canonicalize.html","id":null,"dir":"Reference","previous_headings":"","what":"Deduplication Functions — canonicalize","title":"Deduplication Functions — canonicalize","text":"Functions semantic deduplication extracted records using embeddings Canonicalize text field embedding","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/canonicalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Deduplication Functions — canonicalize","text":"","code":"canonicalize(text)"},{"path":"https://n8layman.github.io/ecoextract/reference/canonicalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Deduplication Functions — canonicalize","text":"text Character vector normalize","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/canonicalize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Deduplication Functions — canonicalize","text":"Normalized character vector","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/canonicalize.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Deduplication Functions — canonicalize","text":"Normalizes text fields improve embedding consistency: - Unicode normalization (NFC) - Lowercase - Trim whitespace","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/check_api_keys_for_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Check that API keys exist for specified models — check_api_keys_for_models","title":"Check that API keys exist for specified models — check_api_keys_for_models","text":"Check API keys exist specified models","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/check_api_keys_for_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check that API keys exist for specified models — check_api_keys_for_models","text":"","code":"check_api_keys_for_models(models)"},{"path":"https://n8layman.github.io/ecoextract/reference/check_api_keys_for_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check that API keys exist for specified models — check_api_keys_for_models","text":"models Character vector model names","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/check_api_keys_for_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check that API keys exist for specified models — check_api_keys_for_models","text":"NULL (stops error keys missing)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/configure_sqlite_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"Database Functions for EcoExtract Package — configure_sqlite_connection","title":"Database Functions for EcoExtract Package — configure_sqlite_connection","text":"Standalone database operations ecological interaction storage Configure SQLite connection optimal concurrency","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/configure_sqlite_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Database Functions for EcoExtract Package — configure_sqlite_connection","text":"","code":"configure_sqlite_connection(con)"},{"path":"https://n8layman.github.io/ecoextract/reference/configure_sqlite_connection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Database Functions for EcoExtract Package — configure_sqlite_connection","text":"con SQLite database connection","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/configure_sqlite_connection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Database Functions for EcoExtract Package — configure_sqlite_connection","text":"connection object (invisibly)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/configure_sqlite_connection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Database Functions for EcoExtract Package — configure_sqlite_connection","text":"Sets PRAGMA options prevent database locked errors enable Write-Ahead Logging (WAL) better concurrent access.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/cosine_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate cosine similarity between two vectors — cosine_similarity","title":"Calculate cosine similarity between two vectors — cosine_similarity","text":"Calculate cosine similarity two vectors","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/cosine_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate cosine similarity between two vectors — cosine_similarity","text":"","code":"cosine_similarity(vec1, vec2)"},{"path":"https://n8layman.github.io/ecoextract/reference/cosine_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate cosine similarity between two vectors — cosine_similarity","text":"vec1 Numeric vector (embedding) vec2 Numeric vector (embedding)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/cosine_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate cosine similarity between two vectors — cosine_similarity","text":"Numeric similarity score (0-1)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/deduplicate_records.html","id":null,"dir":"Reference","previous_headings":"","what":"Deduplicate records using semantic similarity — deduplicate_records","title":"Deduplicate records using semantic similarity — deduplicate_records","text":"Compares new records existing records using embeddings composite keys. inserts records match existing records similarity threshold.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/deduplicate_records.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Deduplicate records using semantic similarity — deduplicate_records","text":"","code":"deduplicate_records(   new_records,   existing_records,   schema_list,   min_similarity = 0.9,   embedding_provider = \"mistral\",   similarity_method = \"llm\",   model = \"anthropic/claude-sonnet-4-5\" )"},{"path":"https://n8layman.github.io/ecoextract/reference/deduplicate_records.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Deduplicate records using semantic similarity — deduplicate_records","text":"new_records Dataframe newly extracted records existing_records Dataframe existing records database schema_list Parsed JSON schema (list) containing required fields min_similarity Minimum cosine similarity consider duplicate (default: 0.9) embedding_provider Provider embeddings (default: \"mistral\") similarity_method Method similarity calculation: \"embedding\", \"jaccard\", \"llm\" (default: \"llm\") model LLM model llm method (default: \"anthropic/claude-sonnet-4-5\")","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/deduplicate_records.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Deduplicate records using semantic similarity — deduplicate_records","text":"List deduplicated records metadata","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/detect_config_source.html","id":null,"dir":"Reference","previous_headings":"","what":"Detect Configuration File Source — detect_config_source","title":"Detect Configuration File Source — detect_config_source","text":"Determines config file loaded without loading . Returns list source type path.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/detect_config_source.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detect Configuration File Source — detect_config_source","text":"","code":"detect_config_source(   file_path = NULL,   file_name = NULL,   package_subdir = \"extdata\" )"},{"path":"https://n8layman.github.io/ecoextract/reference/detect_config_source.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detect Configuration File Source — detect_config_source","text":"file_path Explicit path file (highest priority) file_name Base filename search package_subdir Subdirectory package inst/","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/detect_config_source.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detect Configuration File Source — detect_config_source","text":"List source (\"explicit\", \"project\", \"wd\", \"package\") path","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/diff_records.html","id":null,"dir":"Reference","previous_headings":"","what":"Diff Records Between Original and Edited Versions — diff_records","title":"Diff Records Between Original and Edited Versions — diff_records","text":"Compares two record dataframes categorizes changes id (surrogate key). Uses id stable identification since record_id mutable business identifier.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/diff_records.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Diff Records Between Original and Edited Versions — diff_records","text":"","code":"diff_records(original_df, records_df)"},{"path":"https://n8layman.github.io/ecoextract/reference/diff_records.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Diff Records Between Original and Edited Versions — diff_records","text":"original_df Original records dataframe (edits) records_df Edited records dataframe (edits)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/diff_records.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Diff Records Between Original and Edited Versions — diff_records","text":"List : $modified (ids), $added (dataframe), $deleted (ids)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/ecoextract-package.html","id":null,"dir":"Reference","previous_headings":"","what":"ecoextract: Ecological Data Extraction and Refinement — ecoextract-package","title":"ecoextract: Ecological Data Extraction and Refinement — ecoextract-package","text":"Structured ecological data extraction refinement scientific literature. Provides functions OCR processing, extracting structured data documents, refining extracted data using LLMs.","code":""},{"path":[]},{"path":"https://n8layman.github.io/ecoextract/reference/ecoextract-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"ecoextract: Ecological Data Extraction and Refinement — ecoextract-package","text":"Maintainer: Nathan C Layman n8layman@gmail.com","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/embed_images_in_markdown.html","id":null,"dir":"Reference","previous_headings":"","what":"Embed Images in Markdown (Internal Helper) — embed_images_in_markdown","title":"Embed Images in Markdown (Internal Helper) — embed_images_in_markdown","text":"Replace markdown image bibliography HTML img tags containing base64 data","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/embed_images_in_markdown.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Embed Images in Markdown (Internal Helper) — embed_images_in_markdown","text":"","code":"embed_images_in_markdown(markdown_text, images_data, page_num = 1)"},{"path":"https://n8layman.github.io/ecoextract/reference/embed_images_in_markdown.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Embed Images in Markdown (Internal Helper) — embed_images_in_markdown","text":"markdown_text Markdown text images_data Parsed images JSON object page_num Page number process (default: 1, \"\" pages)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/embed_images_in_markdown.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Embed Images in Markdown (Internal Helper) — embed_images_in_markdown","text":"Processed markdown embedded images","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/export_bibtex.html","id":null,"dir":"Reference","previous_headings":"","what":"Export Bibliography to BibTeX Format — export_bibtex","title":"Export Bibliography to BibTeX Format — export_bibtex","text":"Converts bibliography entries database BibTeX format use LaTeX reference managers. Can export either document metadata (papers ) extracted citations (references papers).","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/export_bibtex.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Export Bibliography to BibTeX Format — export_bibtex","text":"","code":"export_bibtex(   db_conn,   document_ids = NULL,   filename = NULL,   source = c(\"documents\", \"citations\") )"},{"path":"https://n8layman.github.io/ecoextract/reference/export_bibtex.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Export Bibliography to BibTeX Format — export_bibtex","text":"db_conn Database connection path SQLite database file document_ids Optional vector document IDs export (default: documents) filename Optional output file path (e.g., \"references.bib\"). NULL, returns BibTeX string without writing file. source export: \"documents\" exports metadata papers , \"citations\" exports references extracted papers (stored bibliography field). Default: \"documents\".","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/export_bibtex.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Export Bibliography to BibTeX Format — export_bibtex","text":"Character string containing BibTeX entries (invisibly filename provided)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/export_bibtex.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Export Bibliography to BibTeX Format — export_bibtex","text":"","code":"if (FALSE) { # \\dontrun{ # Export document metadata (the papers themselves) export_bibtex(db_conn = \"records.db\", filename = \"papers.bib\")  # Export extracted citations from papers export_bibtex(db_conn = \"records.db\", source = \"citations\",               filename = \"citations.bib\")  # Export citations from specific documents export_bibtex(db_conn = \"records.db\", document_ids = c(1, 5, 10),               source = \"citations\")  # Get BibTeX as string bib_text <- export_bibtex(db_conn = \"records.db\") cat(bib_text) } # }"},{"path":"https://n8layman.github.io/ecoextract/reference/export_citations_bibtex.html","id":null,"dir":"Reference","previous_headings":"","what":"Export Citations from Bibliography Field — export_citations_bibtex","title":"Export Citations from Bibliography Field — export_citations_bibtex","text":"Exports citations extracted papers (stored bibliography column) BibTeX format.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/export_citations_bibtex.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Export Citations from Bibliography Field — export_citations_bibtex","text":"","code":"export_citations_bibtex(con, document_ids = NULL, filename = NULL)"},{"path":"https://n8layman.github.io/ecoextract/reference/export_citations_bibtex.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Export Citations from Bibliography Field — export_citations_bibtex","text":"con Database connection object document_ids Optional vector document IDs filename Optional output file path","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/export_citations_bibtex.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Export Citations from Bibliography Field — export_citations_bibtex","text":"Character string containing BibTeX entries","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/export_db.html","id":null,"dir":"Reference","previous_headings":"","what":"Export Database — export_db","title":"Export Database — export_db","text":"Export records joined document metadata","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/export_db.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Export Database — export_db","text":"","code":"export_db(   document_id = NULL,   db_conn = \"ecoextract_records.db\",   include_ocr = FALSE,   simple = FALSE,   filename = NULL )"},{"path":"https://n8layman.github.io/ecoextract/reference/export_db.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Export Database — export_db","text":"document_id Optional document ID filter (NULL documents) db_conn Database connection (DBI backend) path SQLite database file. Defaults \"ecoextract_records.db\" include_ocr TRUE, include OCR content export (default: FALSE) simple TRUE, exclude processing metadata columns (default: FALSE) filename Optional path save CSV file (NULL, returns tibble )","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/export_db.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Export Database — export_db","text":"Tibble records joined document metadata, invisibly saved file","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/export_db.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Export Database — export_db","text":"","code":"if (FALSE) { # \\dontrun{ # Get all records with metadata as tibble data <- export_db()  # Get records for specific document data <- export_db(document_id = 1)  # Export to CSV export_db(filename = \"extracted_data.csv\")  # Include OCR content data <- export_db(include_ocr = TRUE)  # Simplified output (no processing metadata) data <- export_db(simple = TRUE) } # }"},{"path":"https://n8layman.github.io/ecoextract/reference/extract_fields_from_json_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract field definitions from JSON schema — extract_fields_from_json_schema","title":"Extract field definitions from JSON schema — extract_fields_from_json_schema","text":"Extract field definitions JSON schema","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/extract_fields_from_json_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract field definitions from JSON schema — extract_fields_from_json_schema","text":"","code":"extract_fields_from_json_schema(schema_json_list)"},{"path":"https://n8layman.github.io/ecoextract/reference/extract_fields_from_json_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract field definitions from JSON schema — extract_fields_from_json_schema","text":"schema_json_list Parsed JSON schema list","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/extract_fields_from_json_schema.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract field definitions from JSON schema — extract_fields_from_json_schema","text":"Named list field names, types, requirements","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/extract_metadata.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Publication Metadata — extract_metadata","title":"Extract Publication Metadata — extract_metadata","text":"Extracts publication metadata OCR-processed scientific documents: - Title, authors, publication year, DOI, journal - Saves results documents table","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/extract_metadata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Publication Metadata — extract_metadata","text":"","code":"extract_metadata(   document_id,   db_conn,   force_reprocess = TRUE,   model = \"anthropic/claude-sonnet-4-5\" )"},{"path":"https://n8layman.github.io/ecoextract/reference/extract_metadata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Publication Metadata — extract_metadata","text":"document_id Document ID database db_conn Database connection force_reprocess Ignored (kept backward compatibility). Skip logic handled workflow. model LLM model metadata extraction (default: \"anthropic/claude-sonnet-4-5\")","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/extract_metadata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Publication Metadata — extract_metadata","text":"List status (\"completed\"/<error message>) document_id","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/extract_metadata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Publication Metadata — extract_metadata","text":"schema-agnostic step extracts universal publication metadata regardless domain-specific extraction schema used later steps. Skip logic handled workflow - function always runs called.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/extract_records.html","id":null,"dir":"Reference","previous_headings":"","what":"Ecological Data Extraction Functions — extract_records","title":"Ecological Data Extraction Functions — extract_records","text":"Extract structured ecological interaction data OCR-processed documents Extract records markdown text","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/extract_records.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ecological Data Extraction Functions — extract_records","text":"","code":"extract_records(   document_id = NA,   db_conn = NA,   document_content = NA,   extraction_prompt_file = NULL,   extraction_context_file = NULL,   schema_file = NULL,   model = \"anthropic/claude-sonnet-4-5\",   min_similarity = 0.9,   embedding_provider = \"openai\",   similarity_method = \"llm\",   ... )"},{"path":"https://n8layman.github.io/ecoextract/reference/extract_records.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ecological Data Extraction Functions — extract_records","text":"document_id Optional document ID context db_conn Optional path interaction database document_content OCR-processed markdown content extraction_prompt_file Path custom extraction prompt file (optional) extraction_context_file Path custom extraction context template file (optional) schema_file Path custom schema JSON file (optional) model Provider model format \"provider/model\" (default: \"anthropic/claude-sonnet-4-5\") min_similarity Minimum similarity deduplication (default: 0.9) embedding_provider Provider embeddings using embedding method (default: \"mistral\") similarity_method Method deduplication similarity: \"embedding\", \"jaccard\", \"llm\" (default: \"llm\") ... Additional arguments passed extraction","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/extract_records.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ecological Data Extraction Functions — extract_records","text":"List extraction results","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/extract_records.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Ecological Data Extraction Functions — extract_records","text":"Skip logic handled workflow - function always runs called. Uses deduplication avoid creating duplicate records.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/generate_columns_from_json_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate SQL column definitions from JSON schema — generate_columns_from_json_schema","title":"Generate SQL column definitions from JSON schema — generate_columns_from_json_schema","text":"Generate SQL column definitions JSON schema","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/generate_columns_from_json_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate SQL column definitions from JSON schema — generate_columns_from_json_schema","text":"","code":"generate_columns_from_json_schema(schema_json_list)"},{"path":"https://n8layman.github.io/ecoextract/reference/generate_columns_from_json_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate SQL column definitions from JSON schema — generate_columns_from_json_schema","text":"schema_json_list Parsed JSON schema list","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/generate_columns_from_json_schema.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate SQL column definitions from JSON schema — generate_columns_from_json_schema","text":"Character string SQL column definitions","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/generate_record_id.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate record ID for a record (internal) — generate_record_id","title":"Generate record ID for a record (internal) — generate_record_id","text":"Generate record ID record (internal)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/generate_record_id.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate record ID for a record (internal) — generate_record_id","text":"","code":"generate_record_id(author_lastname, publication_year, sequence_number = 1)"},{"path":"https://n8layman.github.io/ecoextract/reference/generate_record_id.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate record ID for a record (internal) — generate_record_id","text":"author_lastname Author surname publication_year Publication year sequence_number Sequence number record","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/generate_record_id.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate record ID for a record (internal) — generate_record_id","text":"Character record ID","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_context.html","id":null,"dir":"Reference","previous_headings":"","what":"Get context template from package or custom location (internal) — get_context","title":"Get context template from package or custom location (internal) — get_context","text":"Get context template package custom location (internal)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_context.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get context template from package or custom location (internal) — get_context","text":"","code":"get_context(context_file = NULL, context_type = \"extraction\")"},{"path":"https://n8layman.github.io/ecoextract/reference/get_context.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get context template from package or custom location (internal) — get_context","text":"context_file Optional path custom context template file context_type Type context: \"extraction\" \"refinement\"","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_context.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get context template from package or custom location (internal) — get_context","text":"Character string context template","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_db_stats.html","id":null,"dir":"Reference","previous_headings":"","what":"Get database statistics — get_db_stats","title":"Get database statistics — get_db_stats","text":"Get database statistics","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_db_stats.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get database statistics — get_db_stats","text":"","code":"get_db_stats(db_conn)"},{"path":"https://n8layman.github.io/ecoextract/reference/get_db_stats.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get database statistics — get_db_stats","text":"db_conn Database connection path database file","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_db_stats.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get database statistics — get_db_stats","text":"List database statistics","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_document_content.html","id":null,"dir":"Reference","previous_headings":"","what":"Get document content (OCR results) from database (internal) — get_document_content","title":"Get document content (OCR results) from database (internal) — get_document_content","text":"Get document content (OCR results) database (internal)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_document_content.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get document content (OCR results) from database (internal) — get_document_content","text":"","code":"get_document_content(document_id, db_conn)"},{"path":"https://n8layman.github.io/ecoextract/reference/get_document_content.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get document content (OCR results) from database (internal) — get_document_content","text":"document_id Document ID retrieve db_conn Database connection","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_document_content.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get document content (OCR results) from database (internal) — get_document_content","text":"Character string OCR markdown content, NA found","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_documents.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Documents — get_documents","title":"Get Documents — get_documents","text":"Retrieve documents database","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_documents.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Documents — get_documents","text":"","code":"get_documents(document_id = NULL, db_conn = \"ecoextract_records.db\")"},{"path":"https://n8layman.github.io/ecoextract/reference/get_documents.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Documents — get_documents","text":"document_id Document ID filter (NULL documents) db_conn Database connection (DBI backend) path SQLite database file. Defaults \"ecoextract_records.db\"","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_documents.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Documents — get_documents","text":"Tibble document metadata","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_documents.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Documents — get_documents","text":"","code":"if (FALSE) { # \\dontrun{ # Using default SQLite database all_docs <- get_documents() doc <- get_documents(document_id = 1)  # Using explicit connection db <- DBI::dbConnect(RSQLite::SQLite(), \"ecoextract.sqlite\") all_docs <- get_documents(db_conn = db) doc <- get_documents(document_id = 1, db_conn = db) DBI::dbDisconnect(db) } # }"},{"path":"https://n8layman.github.io/ecoextract/reference/get_existing_records.html","id":null,"dir":"Reference","previous_headings":"","what":"Get existing records from database (internal) — get_existing_records","title":"Get existing records from database (internal) — get_existing_records","text":"Get existing records database (internal)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_existing_records.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get existing records from database (internal) — get_existing_records","text":"","code":"get_existing_records(document_id, db_conn)"},{"path":"https://n8layman.github.io/ecoextract/reference/get_existing_records.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get existing records from database (internal) — get_existing_records","text":"document_id Document ID retrieve records db_conn Database connection","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_existing_records.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get existing records from database (internal) — get_existing_records","text":"Dataframe existing records, NA none found","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_extraction_context_template.html","id":null,"dir":"Reference","previous_headings":"","what":"Get extraction context template (internal) — get_extraction_context_template","title":"Get extraction context template (internal) — get_extraction_context_template","text":"Get extraction context template (internal)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_extraction_context_template.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get extraction context template (internal) — get_extraction_context_template","text":"","code":"get_extraction_context_template(context_file = NULL)"},{"path":"https://n8layman.github.io/ecoextract/reference/get_extraction_context_template.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get extraction context template (internal) — get_extraction_context_template","text":"context_file Optional path custom context template file","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_extraction_context_template.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get extraction context template (internal) — get_extraction_context_template","text":"Character string context template","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_extraction_prompt.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal prompt management functions — get_extraction_prompt","title":"Internal prompt management functions — get_extraction_prompt","text":"Handle system prompts templates ecological data extraction Get extraction prompt package custom location (internal)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_extraction_prompt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal prompt management functions — get_extraction_prompt","text":"","code":"get_extraction_prompt(prompt_file = NULL)"},{"path":"https://n8layman.github.io/ecoextract/reference/get_extraction_prompt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal prompt management functions — get_extraction_prompt","text":"prompt_file Optional path custom extraction prompt file","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_extraction_prompt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal prompt management functions — get_extraction_prompt","text":"Character string extraction prompt","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_metadata_context.html","id":null,"dir":"Reference","previous_headings":"","what":"Get metadata context template (internal) — get_metadata_context","title":"Get metadata context template (internal) — get_metadata_context","text":"Get metadata context template (internal)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_metadata_context.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get metadata context template (internal) — get_metadata_context","text":"","code":"get_metadata_context(context_file = NULL)"},{"path":"https://n8layman.github.io/ecoextract/reference/get_metadata_context.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get metadata context template (internal) — get_metadata_context","text":"context_file Optional path custom context template file","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_metadata_context.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get metadata context template (internal) — get_metadata_context","text":"Character string context template","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_metadata_prompt.html","id":null,"dir":"Reference","previous_headings":"","what":"Get metadata prompt from package or custom location (internal) — get_metadata_prompt","title":"Get metadata prompt from package or custom location (internal) — get_metadata_prompt","text":"Get metadata prompt package custom location (internal)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_metadata_prompt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get metadata prompt from package or custom location (internal) — get_metadata_prompt","text":"","code":"get_metadata_prompt(prompt_file = NULL)"},{"path":"https://n8layman.github.io/ecoextract/reference/get_metadata_prompt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get metadata prompt from package or custom location (internal) — get_metadata_prompt","text":"prompt_file Optional path custom metadata prompt file","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_metadata_prompt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get metadata prompt from package or custom location (internal) — get_metadata_prompt","text":"Character string metadata prompt","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_ocr_html_preview.html","id":null,"dir":"Reference","previous_headings":"","what":"Get OCR HTML Preview — get_ocr_html_preview","title":"Get OCR HTML Preview — get_ocr_html_preview","text":"Render OCR results HTML embedded images","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_ocr_html_preview.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get OCR HTML Preview — get_ocr_html_preview","text":"","code":"get_ocr_html_preview(   document_id,   db_conn = \"ecoextract_records.db\",   page_num = 1 )"},{"path":"https://n8layman.github.io/ecoextract/reference/get_ocr_html_preview.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get OCR HTML Preview — get_ocr_html_preview","text":"document_id Document ID db_conn Database connection (DBI backend) path SQLite database file. Defaults \"ecoextract_records.db\" page_num Page number render (default: 1, use \"\" pages)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_ocr_html_preview.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get OCR HTML Preview — get_ocr_html_preview","text":"Browsable HTML object display RStudio viewer","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_ocr_html_preview.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get OCR HTML Preview — get_ocr_html_preview","text":"","code":"if (FALSE) { # \\dontrun{ # Using default SQLite database html <- get_ocr_html_preview(1) print(html)  # Opens in RStudio viewer  # Using explicit connection db <- DBI::dbConnect(RSQLite::SQLite(), \"ecoextract.sqlite\") html <- get_ocr_html_preview(1, db) print(html)  # Opens in RStudio viewer DBI::dbDisconnect(db) } # }"},{"path":"https://n8layman.github.io/ecoextract/reference/get_ocr_markdown.html","id":null,"dir":"Reference","previous_headings":"","what":"Data Access Functions — get_ocr_markdown","title":"Data Access Functions — get_ocr_markdown","text":"Functions retrieving OCR results, audit data, extracted records database Get OCR Markdown","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_ocr_markdown.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data Access Functions — get_ocr_markdown","text":"","code":"get_ocr_markdown(document_id, db_conn = \"ecoextract_records.db\")"},{"path":"https://n8layman.github.io/ecoextract/reference/get_ocr_markdown.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Data Access Functions — get_ocr_markdown","text":"document_id Document ID db_conn Database connection (DBI backend) path SQLite database file. Defaults \"ecoextract_records.db\"","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_ocr_markdown.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Data Access Functions — get_ocr_markdown","text":"Character string markdown content, NA found","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_ocr_markdown.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Data Access Functions — get_ocr_markdown","text":"Retrieve OCR markdown text document","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_ocr_markdown.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Data Access Functions — get_ocr_markdown","text":"","code":"if (FALSE) { # \\dontrun{ # Using default SQLite database markdown <- get_ocr_markdown(1)  # Using explicit connection db <- DBI::dbConnect(RSQLite::SQLite(), \"ecoextract.sqlite\") markdown <- get_ocr_markdown(1, db) DBI::dbDisconnect(db) } # }"},{"path":"https://n8layman.github.io/ecoextract/reference/get_record_columns_sql.html","id":null,"dir":"Reference","previous_headings":"","what":"Get record table column definitions as SQL — get_record_columns_sql","title":"Get record table column definitions as SQL — get_record_columns_sql","text":"Get record table column definitions SQL","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_record_columns_sql.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get record table column definitions as SQL — get_record_columns_sql","text":"","code":"get_record_columns_sql(schema_json_list)"},{"path":"https://n8layman.github.io/ecoextract/reference/get_record_columns_sql.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get record table column definitions as SQL — get_record_columns_sql","text":"schema_json_list Parsed JSON schema generate columns ","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_record_columns_sql.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get record table column definitions as SQL — get_record_columns_sql","text":"Character string column definitions","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_records.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Records — get_records","title":"Get Records — get_records","text":"Retrieve extracted records database","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_records.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Records — get_records","text":"","code":"get_records(document_id = NULL, db_conn = \"ecoextract_records.db\")"},{"path":"https://n8layman.github.io/ecoextract/reference/get_records.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Records — get_records","text":"document_id Document ID filter (NULL records) db_conn Database connection (DBI backend) path SQLite database file. Defaults \"ecoextract_records.db\"","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_records.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Records — get_records","text":"Tibble records","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_records.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Records — get_records","text":"","code":"if (FALSE) { # \\dontrun{ # Using default SQLite database all_records <- get_records() doc_records <- get_records(document_id = 1)  # Using explicit connection db <- DBI::dbConnect(RSQLite::SQLite(), \"ecoextract.sqlite\") all_records <- get_records(db_conn = db) doc_records <- get_records(document_id = 1, db_conn = db) DBI::dbDisconnect(db) } # }"},{"path":"https://n8layman.github.io/ecoextract/reference/get_refinement_context_template.html","id":null,"dir":"Reference","previous_headings":"","what":"Get refinement context template (internal) — get_refinement_context_template","title":"Get refinement context template (internal) — get_refinement_context_template","text":"Get refinement context template (internal)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_refinement_context_template.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get refinement context template (internal) — get_refinement_context_template","text":"","code":"get_refinement_context_template(context_file = NULL)"},{"path":"https://n8layman.github.io/ecoextract/reference/get_refinement_context_template.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get refinement context template (internal) — get_refinement_context_template","text":"context_file Optional path custom context template file","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_refinement_context_template.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get refinement context template (internal) — get_refinement_context_template","text":"Character string context template","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_refinement_prompt.html","id":null,"dir":"Reference","previous_headings":"","what":"Get refinement prompt from package or custom location (internal) — get_refinement_prompt","title":"Get refinement prompt from package or custom location (internal) — get_refinement_prompt","text":"Get refinement prompt package custom location (internal)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_refinement_prompt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get refinement prompt from package or custom location (internal) — get_refinement_prompt","text":"","code":"get_refinement_prompt(prompt_file = NULL)"},{"path":"https://n8layman.github.io/ecoextract/reference/get_refinement_prompt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get refinement prompt from package or custom location (internal) — get_refinement_prompt","text":"prompt_file Optional path custom refinement prompt file","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_refinement_prompt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get refinement prompt from package or custom location (internal) — get_refinement_prompt","text":"Character string refinement prompt","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_schema_array_fields.html","id":null,"dir":"Reference","previous_headings":"","what":"Get array field names from schema definition — get_schema_array_fields","title":"Get array field names from schema definition — get_schema_array_fields","text":"Identifies fields defined arrays JSON schema.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_schema_array_fields.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get array field names from schema definition — get_schema_array_fields","text":"","code":"get_schema_array_fields(schema_list)"},{"path":"https://n8layman.github.io/ecoextract/reference/get_schema_array_fields.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get array field names from schema definition — get_schema_array_fields","text":"schema_list Parsed JSON schema (jsonlite::fromJSON)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/get_schema_array_fields.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get array field names from schema definition — get_schema_array_fields","text":"Character vector array field names","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/init_ecoextract.html","id":null,"dir":"Reference","previous_headings":"","what":"Initialize ecoextract Project Configuration — init_ecoextract","title":"Initialize ecoextract Project Configuration — init_ecoextract","text":"Creates ecoextract/ directory project root copies default template files customization. allows users override package defaults per-project basis.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/init_ecoextract.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initialize ecoextract Project Configuration — init_ecoextract","text":"","code":"init_ecoextract(project_dir = getwd(), overwrite = FALSE)"},{"path":"https://n8layman.github.io/ecoextract/reference/init_ecoextract.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initialize ecoextract Project Configuration — init_ecoextract","text":"project_dir Directory create ecoextract/ folder (default: current directory) overwrite Whether overwrite existing files","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/init_ecoextract.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Initialize ecoextract Project Configuration — init_ecoextract","text":"Invisibly returns TRUE successful","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/init_ecoextract.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Initialize ecoextract Project Configuration — init_ecoextract","text":"","code":"if (FALSE) { # \\dontrun{ # Create ecoextract config directory with templates init_ecoextract()  # Now customize files in ecoextract/ directory: # - Read SCHEMA_GUIDE.md for schema format requirements # - Edit schema.json to define your data fields # - Edit extraction_prompt.md to describe what to extract } # }"},{"path":"https://n8layman.github.io/ecoextract/reference/init_ecoextract_database.html","id":null,"dir":"Reference","previous_headings":"","what":"Initialize EcoExtract database — init_ecoextract_database","title":"Initialize EcoExtract database — init_ecoextract_database","text":"Initialize EcoExtract database","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/init_ecoextract_database.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initialize EcoExtract database — init_ecoextract_database","text":"","code":"init_ecoextract_database(   db_conn = \"ecoextract_results.sqlite\",   schema_file = NULL )"},{"path":"https://n8layman.github.io/ecoextract/reference/init_ecoextract_database.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initialize EcoExtract database — init_ecoextract_database","text":"db_conn Database connection (DBI backend) path SQLite database file schema_file Optional path JSON schema file (determines record columns)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/init_ecoextract_database.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Initialize EcoExtract database — init_ecoextract_database","text":"NULL (creates database required tables)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/is_forced.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if a document should be forced to reprocess — is_forced","title":"Check if a document should be forced to reprocess — is_forced","text":"Check document forced reprocess","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/is_forced.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if a document should be forced to reprocess — is_forced","text":"","code":"is_forced(force_param, document_id)"},{"path":"https://n8layman.github.io/ecoextract/reference/is_forced.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if a document should be forced to reprocess — is_forced","text":"force_param NULL, TRUE, integer vector document_ids document_id document ID check","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/is_forced.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if a document should be forced to reprocess — is_forced","text":"logical","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/jaccard_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Jaccard similarity between two strings — jaccard_similarity","title":"Calculate Jaccard similarity between two strings — jaccard_similarity","text":"Tokenizes strings character n-grams calculates Jaccard similarity. fast, non-API-dependent method string comparison.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/jaccard_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Jaccard similarity between two strings — jaccard_similarity","text":"","code":"jaccard_similarity(str1, str2, n = 3)"},{"path":"https://n8layman.github.io/ecoextract/reference/jaccard_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Jaccard similarity between two strings — jaccard_similarity","text":"str1 First string str2 Second string n N-gram size (default: 3 trigrams)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/jaccard_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Jaccard similarity between two strings — jaccard_similarity","text":"Numeric similarity score (0-1)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/json_schema_to_ellmer_type_metadata.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert metadata JSON schema to native ellmer type specification — json_schema_to_ellmer_type_metadata","title":"Convert metadata JSON schema to native ellmer type specification — json_schema_to_ellmer_type_metadata","text":"Converts metadata schema ellmer's native type functions proper dataframe conversion","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/json_schema_to_ellmer_type_metadata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert metadata JSON schema to native ellmer type specification — json_schema_to_ellmer_type_metadata","text":"","code":"json_schema_to_ellmer_type_metadata(schema_path)"},{"path":"https://n8layman.github.io/ecoextract/reference/json_schema_to_ellmer_type_metadata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert metadata JSON schema to native ellmer type specification — json_schema_to_ellmer_type_metadata","text":"schema_path Path metadata JSON schema file","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/json_schema_to_ellmer_type_metadata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert metadata JSON schema to native ellmer type specification — json_schema_to_ellmer_type_metadata","text":"Ellmer type specification object","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/limit_to_first_n_pages.html","id":null,"dir":"Reference","previous_headings":"","what":"Limit document content to first N pages — limit_to_first_n_pages","title":"Limit document content to first N pages — limit_to_first_n_pages","text":"Limit document content first N pages","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/limit_to_first_n_pages.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Limit document content to first N pages — limit_to_first_n_pages","text":"","code":"limit_to_first_n_pages(content, n = 3)"},{"path":"https://n8layman.github.io/ecoextract/reference/limit_to_first_n_pages.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Limit document content to first N pages — limit_to_first_n_pages","text":"content Full document content (markdown OCR) n Number pages keep (default: 3)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/limit_to_first_n_pages.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Limit document content to first N pages — limit_to_first_n_pages","text":"Content limited first n pages","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/llm_deduplicate.html","id":null,"dir":"Reference","previous_headings":"","what":"LLM-based deduplication — llm_deduplicate","title":"LLM-based deduplication — llm_deduplicate","text":"Compare new records existing records using LLM. Returns indices new records duplicates. standalone function dependencies ecoextract code.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/llm_deduplicate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"LLM-based deduplication — llm_deduplicate","text":"","code":"llm_deduplicate(   new_records,   existing_records,   key_fields,   model = \"anthropic/claude-sonnet-4-5\" )"},{"path":"https://n8layman.github.io/ecoextract/reference/llm_deduplicate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"LLM-based deduplication — llm_deduplicate","text":"new_records Dataframe new records existing_records Dataframe existing records key_fields Character vector column names compare model LLM model (default: \"anthropic/claude-sonnet-4-5\")","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/llm_deduplicate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"LLM-based deduplication — llm_deduplicate","text":"Integer vector 1-based indices unique new records","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/load_config_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Load Configuration File with Priority Order — load_config_file","title":"Load Configuration File with Priority Order — load_config_file","text":"Searches configuration files following priority order: 1. Explicit file path (provided) 2. Project ecoextract/ directory 3. Working directory (ecoextract_ prefix) 4. Package default location","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/load_config_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load Configuration File with Priority Order — load_config_file","text":"","code":"load_config_file(   file_path = NULL,   file_name = NULL,   package_subdir = \"extdata\",   return_content = FALSE )"},{"path":"https://n8layman.github.io/ecoextract/reference/load_config_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load Configuration File with Priority Order — load_config_file","text":"file_path Explicit path file (highest priority) file_name Base filename search (e.g., \"schema.json\", \"extraction_prompt.md\") package_subdir Subdirectory package inst/ (e.g., \"extdata\", \"prompts\") return_content TRUE, returns file content; FALSE, returns file path","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/load_config_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load Configuration File with Priority Order — load_config_file","text":"File path content, depending return_content parameter","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/log_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Simple logging function — log_message","title":"Simple logging function — log_message","text":"Simple logging function","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/log_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simple logging function — log_message","text":"","code":"log_message(message, level = \"INFO\")"},{"path":"https://n8layman.github.io/ecoextract/reference/log_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simple logging function — log_message","text":"message Message log level Log level (INFO, WARNING, ERROR)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/make_unique_keys.html","id":null,"dir":"Reference","previous_headings":"","what":"Make Citation Keys Unique — make_unique_keys","title":"Make Citation Keys Unique — make_unique_keys","text":"Handles duplicate citation keys appending letters (, b, c, etc.)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/make_unique_keys.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make Citation Keys Unique — make_unique_keys","text":"","code":"make_unique_keys(bibtex_entries)"},{"path":"https://n8layman.github.io/ecoextract/reference/make_unique_keys.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make Citation Keys Unique — make_unique_keys","text":"bibtex_entries Character vector BibTeX entries","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/make_unique_keys.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make Citation Keys Unique — make_unique_keys","text":"Character vector unique citation keys","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/match_and_restore_record_ids.html","id":null,"dir":"Reference","previous_headings":"","what":"Restore id and record_id from existing records after LLM refinement — match_and_restore_record_ids","title":"Restore id and record_id from existing records after LLM refinement — match_and_restore_record_ids","text":"Restore id record_id existing records LLM refinement","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/match_and_restore_record_ids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Restore id and record_id from existing records after LLM refinement — match_and_restore_record_ids","text":"","code":"match_and_restore_record_ids(refined_records, existing_records)"},{"path":"https://n8layman.github.io/ecoextract/reference/match_and_restore_record_ids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Restore id and record_id from existing records after LLM refinement — match_and_restore_record_ids","text":"refined_records Dataframe records LLM refinement existing_records Dataframe existing records database (must include id)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/match_and_restore_record_ids.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Restore id and record_id from existing records after LLM refinement — match_and_restore_record_ids","text":"Dataframe id restored via join record_id","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/merge_refinements.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge refined data back into original records (internal) — merge_refinements","title":"Merge refined data back into original records (internal) — merge_refinements","text":"Merge refined data back original records (internal)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/merge_refinements.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge refined data back into original records (internal) — merge_refinements","text":"","code":"merge_refinements(original_records, refined_records)"},{"path":"https://n8layman.github.io/ecoextract/reference/merge_refinements.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge refined data back into original records (internal) — merge_refinements","text":"original_records Dataframe original records refined_records Dataframe refined records","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/merge_refinements.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge refined data back into original records (internal) — merge_refinements","text":"Dataframe merged refinements","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/migrate_database.html","id":null,"dir":"Reference","previous_headings":"","what":"Migrate database schema for existing databases — migrate_database","title":"Migrate database schema for existing databases — migrate_database","text":"Adds new columns tables required schema updates. Safe run multiple times - adds missing elements.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/migrate_database.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Migrate database schema for existing databases — migrate_database","text":"","code":"migrate_database(con)"},{"path":"https://n8layman.github.io/ecoextract/reference/migrate_database.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Migrate database schema for existing databases — migrate_database","text":"con Database connection","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/migrate_database.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Migrate database schema for existing databases — migrate_database","text":"NULL (invisibly)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/normalize_array_fields.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate required array fields based on schema definition (schema-agnostic) — normalize_array_fields","title":"Validate required array fields based on schema definition (schema-agnostic) — normalize_array_fields","text":"Validates required array fields values. Array normalization (flattening nested lists) handled serialization save_records_to_db avoid vctrs type-checking issues tibble column assignment.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/normalize_array_fields.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate required array fields based on schema definition (schema-agnostic) — normalize_array_fields","text":"","code":"normalize_array_fields(df, schema_list)"},{"path":"https://n8layman.github.io/ecoextract/reference/normalize_array_fields.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate required array fields based on schema definition (schema-agnostic) — normalize_array_fields","text":"df Dataframe extracted records schema_list Parsed JSON schema (jsonlite::fromJSON)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/normalize_array_fields.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate required array fields based on schema definition (schema-agnostic) — normalize_array_fields","text":"Validated dataframe (unmodified)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/ocr_document.html","id":null,"dir":"Reference","previous_headings":"","what":"OCR Document and Save to Database — ocr_document","title":"OCR Document and Save to Database — ocr_document","text":"Performs OCR PDF saves document database. Skip logic handled workflow - function always runs OCR called.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/ocr_document.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"OCR Document and Save to Database — ocr_document","text":"","code":"ocr_document(   pdf_file,   db_conn,   force_reprocess = TRUE,   provider = \"tensorlake\",   timeout = 60,   max_wait_seconds = NULL )"},{"path":"https://n8layman.github.io/ecoextract/reference/ocr_document.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"OCR Document and Save to Database — ocr_document","text":"pdf_file Path PDF file db_conn Database connection force_reprocess Ignored (kept backward compatibility). Skip logic handled workflow. provider OCR provider use (default: \"tensorlake\") timeout Maximum seconds wait OCR completion (default: 60) max_wait_seconds Deprecated. Use timeout instead.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/ocr_document.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"OCR Document and Save to Database — ocr_document","text":"List status (\"completed\"/<error message>) document_id","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/parse_citation_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse Citation Text to Extract Fields — parse_citation_text","title":"Parse Citation Text to Extract Fields — parse_citation_text","text":"Attempts extract structured fields citation string using heuristic pattern matching.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/parse_citation_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse Citation Text to Extract Fields — parse_citation_text","text":"","code":"parse_citation_text(citation_text)"},{"path":"https://n8layman.github.io/ecoextract/reference/parse_citation_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse Citation Text to Extract Fields — parse_citation_text","text":"citation_text Character string containing citation","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/parse_citation_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse Citation Text to Extract Fields — parse_citation_text","text":"List extracted fields (author, title, journal, year, etc.)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/perform_ocr.html","id":null,"dir":"Reference","previous_headings":"","what":"OCR Functions — perform_ocr","title":"OCR Functions — perform_ocr","text":"Functions performing OCR PDF documents Perform OCR PDF","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/perform_ocr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"OCR Functions — perform_ocr","text":"","code":"perform_ocr(pdf_file, provider = \"tensorlake\", timeout = 60)"},{"path":"https://n8layman.github.io/ecoextract/reference/perform_ocr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"OCR Functions — perform_ocr","text":"pdf_file Path PDF provider OCR provider use (default: \"tensorlake\") timeout Maximum seconds wait OCR completion (default: 60)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/perform_ocr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"OCR Functions — perform_ocr","text":"List markdown content, images, raw result","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/process_documents.html","id":null,"dir":"Reference","previous_headings":"","what":"Complete Document Processing Workflow — process_documents","title":"Complete Document Processing Workflow — process_documents","text":"Process PDFs complete pipeline: OCR → Metadata → Extract → Refine","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/process_documents.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Complete Document Processing Workflow — process_documents","text":"","code":"process_documents(   pdf_path,   db_conn = \"ecoextract_records.db\",   schema_file = NULL,   extraction_prompt_file = NULL,   refinement_prompt_file = NULL,   model = \"anthropic/claude-sonnet-4-5\",   ocr_provider = \"tensorlake\",   ocr_timeout = 60,   force_reprocess_ocr = NULL,   force_reprocess_metadata = NULL,   force_reprocess_extraction = NULL,   run_extraction = TRUE,   run_refinement = NULL,   min_similarity = 0.9,   embedding_provider = \"openai\",   similarity_method = \"llm\",   recursive = FALSE,   workers = NULL,   log = FALSE,   ... )"},{"path":"https://n8layman.github.io/ecoextract/reference/process_documents.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Complete Document Processing Workflow — process_documents","text":"pdf_path Path single PDF file directory PDFs db_conn Database connection (DBI backend) path SQLite database file. path provided, creates SQLite database exist. connection provided, tables must already exist (use init_ecoextract_database() first). schema_file Optional custom schema file extraction_prompt_file Optional custom extraction prompt refinement_prompt_file Optional custom refinement prompt model LLM model(s) use metadata extraction, record extraction, refinement. Can single model name (character string) vector models tiered fallback. vector provided, models tried sequentially one succeeds. Default: \"anthropic/claude-sonnet-4-5\". Examples: \"openai/gpt-4o\", c(\"anthropic/claude-sonnet-4-5\", \"mistral/mistral-large-latest\") ocr_provider OCR provider use (default: \"tensorlake\"). Options: \"tensorlake\", \"mistral\", \"claude\" ocr_timeout Maximum seconds wait OCR completion (default: 60) force_reprocess_ocr Controls OCR reprocessing. NULL (default) uses normal skip logic, TRUE forces documents, integer vector document_ids force specific documents. force_reprocess_metadata Controls metadata reprocessing. NULL (default) uses normal skip logic, TRUE forces documents, integer vector document_ids force specific documents. force_reprocess_extraction Controls extraction reprocessing. NULL (default) uses normal skip logic, TRUE forces documents, integer vector document_ids force specific documents. run_extraction TRUE, run extraction step find new records. Default TRUE. run_refinement Controls refinement step. NULL (default) skips refinement, TRUE runs documents records, integer vector document_ids refine specific documents. min_similarity Minimum similarity deduplication (default: 0.9) embedding_provider Provider embeddings using embedding method (default: \"openai\") similarity_method Method deduplication similarity: \"embedding\", \"jaccard\", \"llm\" (default: \"llm\") recursive TRUE pdf_path directory, search PDFs subdirectories. Default FALSE. workers Number parallel workers. NULL (default) 1 sequential processing. Values > 1 require crew package db_conn must file path (connection object). log TRUE using parallel processing (workers > 1), write detailed output auto-generated log file (e.g., ecoextract_20240129_143052.log). Default FALSE. Ignored sequential processing. Useful troubleshooting errors. ... Additional arguments (deprecated: use explicit parameters instead)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/process_documents.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Complete Document Processing Workflow — process_documents","text":"Tibble processing results","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/process_documents.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Complete Document Processing Workflow — process_documents","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage - process new PDFs process_documents(\"pdfs/\") process_documents(\"paper.pdf\", \"my_interactions.db\")  # Remote database (Supabase, PostgreSQL, etc.) library(RPostgres) con <- dbConnect(Postgres(),   dbname = \"your_db\",   host = \"db.xxx.supabase.co\",   user = \"postgres\",   password = Sys.getenv(\"SUPABASE_PASSWORD\") ) # Initialize schema first init_ecoextract_database(con) # Then process documents process_documents(\"pdfs/\", db_conn = con) dbDisconnect(con)  # Force re-run OCR for all documents (cascades to metadata and extraction) process_documents(\"pdfs/\", force_reprocess_ocr = TRUE)  # Force re-run OCR for specific documents only process_documents(\"pdfs/\", force_reprocess_ocr = c(5L, 12L))  # Force re-run metadata only (cascades to extraction) process_documents(\"pdfs/\", force_reprocess_metadata = TRUE)  # With custom schema and prompts process_documents(\"pdfs/\", \"interactions.db\",                   schema_file = \"ecoextract/schema.json\",                   extraction_prompt_file = \"ecoextract/extraction_prompt.md\")  # With refinement for all documents process_documents(\"pdfs/\", run_refinement = TRUE)  # Refinement for specific documents only process_documents(\"pdfs/\", run_refinement = c(5L, 12L))  # Skip extraction, refinement only on existing records process_documents(\"pdfs/\", run_extraction = FALSE, run_refinement = TRUE)  # Search for PDFs in all subdirectories process_documents(\"research_papers/\", recursive = TRUE)  # Process in parallel with 4 workers (requires crew package) process_documents(\"pdfs/\", workers = 4)  # Parallel with logging for troubleshooting process_documents(\"pdfs/\", workers = 4, log = TRUE)  # Use different OCR provider process_documents(\"pdfs/\", ocr_provider = \"mistral\")  # Increase OCR timeout to 5 minutes for large documents process_documents(\"pdfs/\", ocr_timeout = 300) } # }"},{"path":"https://n8layman.github.io/ecoextract/reference/process_single_document.html","id":null,"dir":"Reference","previous_headings":"","what":"Process Single Document Through Complete Pipeline — process_single_document","title":"Process Single Document Through Complete Pipeline — process_single_document","text":"Process Single Document Complete Pipeline","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/process_single_document.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process Single Document Through Complete Pipeline — process_single_document","text":"","code":"process_single_document(   pdf_file,   db_conn,   schema_file = NULL,   extraction_prompt_file = NULL,   refinement_prompt_file = NULL,   model = \"anthropic/claude-sonnet-4-5\",   ocr_provider = \"tensorlake\",   ocr_timeout = 60,   force_reprocess_ocr = NULL,   force_reprocess_metadata = NULL,   force_reprocess_extraction = NULL,   run_extraction = TRUE,   run_refinement = NULL,   min_similarity = 0.9,   embedding_provider = \"openai\",   similarity_method = \"llm\",   ... )"},{"path":"https://n8layman.github.io/ecoextract/reference/process_single_document.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process Single Document Through Complete Pipeline — process_single_document","text":"pdf_file Path PDF file db_conn Database connection schema_file Optional custom schema extraction_prompt_file Optional custom extraction prompt refinement_prompt_file Optional custom refinement prompt model LLM model(s) use metadata extraction, record extraction, refinement. Can single model name vector models tiered fallback. Default: \"anthropic/claude-sonnet-4-5\" ocr_provider OCR provider use (default: \"tensorlake\"). Options: \"tensorlake\", \"mistral\", \"claude\". Can also vector fallback. ocr_timeout Maximum seconds wait OCR completion (default: 60) force_reprocess_ocr NULL, TRUE, integer vector document_ids force OCR force_reprocess_metadata NULL, TRUE, integer vector document_ids force metadata force_reprocess_extraction NULL, TRUE, integer vector document_ids force extraction run_extraction TRUE, run extraction step (default: TRUE) run_refinement NULL, TRUE, integer vector document_ids run refinement min_similarity Minimum cosine similarity deduplication (default: 0.9) embedding_provider Provider embeddings (default: \"openai\") similarity_method Method deduplication similarity: \"embedding\", \"jaccard\", \"llm\" (default: \"llm\") ... Additional arguments","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/process_single_document.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process Single Document Through Complete Pipeline — process_single_document","text":"List processing result","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/refine_records.html","id":null,"dir":"Reference","previous_headings":"","what":"Refine extracted records with additional context — refine_records","title":"Refine extracted records with additional context — refine_records","text":"Refine extracted records additional context","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/refine_records.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Refine extracted records with additional context — refine_records","text":"","code":"refine_records(   db_conn = NULL,   document_id,   extraction_prompt_file = NULL,   refinement_prompt_file = NULL,   refinement_context_file = NULL,   schema_file = NULL,   model = \"anthropic/claude-sonnet-4-5\" )"},{"path":"https://n8layman.github.io/ecoextract/reference/refine_records.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Refine extracted records with additional context — refine_records","text":"db_conn Database connection path SQLite database file document_id Document ID extraction_prompt_file Path extraction prompt file (provides domain context) refinement_prompt_file Path custom refinement prompt file (optional, uses generic provided) refinement_context_file Path custom refinement context template file (optional) schema_file Path custom schema JSON file (optional) model Provider model format \"provider/model\" (default: \"anthropic/claude-sonnet-4-5\")","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/refine_records.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Refine extracted records with additional context — refine_records","text":"List refinement results","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/retry_db_operation.html","id":null,"dir":"Reference","previous_headings":"","what":"Retry database operation with exponential backoff — retry_db_operation","title":"Retry database operation with exponential backoff — retry_db_operation","text":"Wraps database operation retry logic handle temporary locks.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/retry_db_operation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retry database operation with exponential backoff — retry_db_operation","text":"","code":"retry_db_operation(expr, max_attempts = 3, initial_wait = 0.5)"},{"path":"https://n8layman.github.io/ecoextract/reference/retry_db_operation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retry database operation with exponential backoff — retry_db_operation","text":"expr Expression evaluate (database operation) max_attempts Maximum number retry attempts (default: 3) initial_wait Initial wait time seconds (default: 0.5)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/retry_db_operation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retry database operation with exponential backoff — retry_db_operation","text":"Result expression","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/save_document.html","id":null,"dir":"Reference","previous_headings":"","what":"Save Document After Human Review — save_document","title":"Save Document After Human Review — save_document","text":"Updates document metadata review timestamp saves modified records, tracking edits record_edits audit table. Designed Shiny app review workflows.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/save_document.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save Document After Human Review — save_document","text":"","code":"save_document(   document_id,   records_df,   original_df = NULL,   db_conn = \"ecoextract_records.db\",   ... )"},{"path":"https://n8layman.github.io/ecoextract/reference/save_document.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save Document After Human Review — save_document","text":"document_id Document ID update records_df Updated records dataframe (Shiny editor) original_df Original records dataframe (edits, diff). NULL, updates reviewed_at timestamp without modifying records. db_conn Database connection path SQLite database file ... Additional metadata fields update document","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/save_document.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save Document After Human Review — save_document","text":"Invisibly returns document_id","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/save_document.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Save Document After Human Review — save_document","text":"","code":"if (FALSE) { # \\dontrun{ # In Shiny app \"Accept\" button handler save_document(   document_id = input$document_select,   records_df = edited_records(),   original_df = original_records(),   db_conn = db_path ) } # }"},{"path":"https://n8layman.github.io/ecoextract/reference/save_document_to_db.html","id":null,"dir":"Reference","previous_headings":"","what":"Save or reprocess a document in the EcoExtract database — save_document_to_db","title":"Save or reprocess a document in the EcoExtract database — save_document_to_db","text":"Inserts new document row `documents` table, automatically computing file hash provided. Optionally, `overwrite = TRUE`, existing document file hash deleted (including associated records), new row preserves old `document_id`.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/save_document_to_db.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save or reprocess a document in the EcoExtract database — save_document_to_db","text":"","code":"save_document_to_db(   db_conn,   file_path,   file_hash = NULL,   metadata = list(),   overwrite = FALSE )"},{"path":"https://n8layman.github.io/ecoextract/reference/save_document_to_db.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save or reprocess a document in the EcoExtract database — save_document_to_db","text":"db_conn DBI connection object path SQLite database file. file_path Path PDF document file store. file_hash Optional precomputed file hash (MD5). `NULL`, computed automatically file. metadata named list document metadata. Recognized keys include: title Document title. first_author_lastname Last name first author. publication_year Year publication. doi DOI document. journal Journal name. document_content OCR processed content. ocr_images OCR images (JSON array similar). overwrite Logical; `TRUE`, existing row file hash deleted new row preserves old `document_id`.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/save_document_to_db.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save or reprocess a document in the EcoExtract database — save_document_to_db","text":"`document_id` inserted replaced row, `NULL` insertion fails.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/save_document_to_db.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Save or reprocess a document in the EcoExtract database — save_document_to_db","text":"","code":"if (FALSE) { # \\dontrun{ db <- \"ecoextract_results.sqlite\" save_document_to_db(db, \"example.pdf\", metadata = list(title = \"My Paper\"), overwrite = TRUE) } # }"},{"path":"https://n8layman.github.io/ecoextract/reference/save_metadata_to_db.html","id":null,"dir":"Reference","previous_headings":"","what":"Save publication metadata to EcoExtract database (internal) — save_metadata_to_db","title":"Save publication metadata to EcoExtract database (internal) — save_metadata_to_db","text":"Updates existing document metadata fields currently NULL/NA/empty. Optionally overwrites metadata `overwrite = TRUE`.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/save_metadata_to_db.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save publication metadata to EcoExtract database (internal) — save_metadata_to_db","text":"","code":"save_metadata_to_db(   document_id,   db_conn,   metadata = list(),   metadata_llm_model = NULL,   metadata_log = NULL,   overwrite = FALSE )"},{"path":"https://n8layman.github.io/ecoextract/reference/save_metadata_to_db.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save publication metadata to EcoExtract database (internal) — save_metadata_to_db","text":"document_id Document ID update db_conn Database connection path SQLite database metadata Named list metadata fields overwrite Logical, TRUE overwrite existing fields","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/save_metadata_to_db.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save publication metadata to EcoExtract database (internal) — save_metadata_to_db","text":"Document ID","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/save_reasoning_to_db.html","id":null,"dir":"Reference","previous_headings":"","what":"Save reasoning to database (internal) — save_reasoning_to_db","title":"Save reasoning to database (internal) — save_reasoning_to_db","text":"Save reasoning database (internal)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/save_reasoning_to_db.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save reasoning to database (internal) — save_reasoning_to_db","text":"","code":"save_reasoning_to_db(   document_id,   db_conn,   reasoning_text,   step = c(\"extraction\", \"refinement\") )"},{"path":"https://n8layman.github.io/ecoextract/reference/save_reasoning_to_db.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save reasoning to database (internal) — save_reasoning_to_db","text":"document_id Document ID db_conn Database connection reasoning_text Reasoning text save step Either \"extraction\" \"refinement\"","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/save_records_to_db.html","id":null,"dir":"Reference","previous_headings":"","what":"Save records to EcoExtract database (internal) — save_records_to_db","title":"Save records to EcoExtract database (internal) — save_records_to_db","text":"Save records EcoExtract database (internal)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/save_records_to_db.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save records to EcoExtract database (internal) — save_records_to_db","text":"","code":"save_records_to_db(   db_path,   document_id,   interactions_df,   metadata = list(),   schema_list = NULL,   mode = \"insert\" )"},{"path":"https://n8layman.github.io/ecoextract/reference/save_records_to_db.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save records to EcoExtract database (internal) — save_records_to_db","text":"db_path Path database file document_id Document ID interactions_df Dataframe records metadata Processing metadata schema_list Optional parsed JSON schema array normalization","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/save_records_to_db.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save records to EcoExtract database (internal) — save_records_to_db","text":"TRUE successful","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/should_run_step.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine if a processing step should run — should_run_step","title":"Determine if a processing step should run — should_run_step","text":"Determine processing step run","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/should_run_step.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine if a processing step should run — should_run_step","text":"","code":"should_run_step(status, data_exists)"},{"path":"https://n8layman.github.io/ecoextract/reference/should_run_step.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine if a processing step should run — should_run_step","text":"status Current status value step data_exists Logical NULL. logical, checks desync (status=\"completed\" data missing). Pass NULL skip desync check.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/should_run_step.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine if a processing step should run — should_run_step","text":"logical - TRUE step run, FALSE skip","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/try_models_with_fallback.html","id":null,"dir":"Reference","previous_headings":"","what":"Try multiple LLM models with fallback on refusal — try_models_with_fallback","title":"Try multiple LLM models with fallback on refusal — try_models_with_fallback","text":"Attempts get structured output LLMs sequential order. model refuses (stop_reason == \"refusal\") errors, tries next model.","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/try_models_with_fallback.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Try multiple LLM models with fallback on refusal — try_models_with_fallback","text":"","code":"try_models_with_fallback(   models,   system_prompt,   context,   schema,   max_tokens = 16384,   step_name = \"LLM call\" )"},{"path":"https://n8layman.github.io/ecoextract/reference/try_models_with_fallback.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Try multiple LLM models with fallback on refusal — try_models_with_fallback","text":"models Character vector model names (e.g., c(\"anthropic/claude-sonnet-4-5\", \"mistral/mistral-large-latest\")) system_prompt System prompt LLM context User context/input LLM schema ellmer type schema structured output max_tokens Maximum tokens response (default 16384) step_name Name step logging (default \"LLM call\")","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/try_models_with_fallback.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Try multiple LLM models with fallback on refusal — try_models_with_fallback","text":"List result (structured output), model_used (model succeeded), error_log (JSON string failed attempts)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/validate_force_param.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate force_reprocess parameter — validate_force_param","title":"Validate force_reprocess parameter — validate_force_param","text":"Validate force_reprocess parameter","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/validate_force_param.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate force_reprocess parameter — validate_force_param","text":"","code":"validate_force_param(param, param_name)"},{"path":"https://n8layman.github.io/ecoextract/reference/validate_force_param.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate force_reprocess parameter — validate_force_param","text":"param parameter value validate param_name Name parameter error messages","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/validate_schema_with_db.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate schema compatibility with database (internal) — validate_schema_with_db","title":"Validate schema compatibility with database (internal) — validate_schema_with_db","text":"Validate schema compatibility database (internal)","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/validate_schema_with_db.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate schema compatibility with database (internal) — validate_schema_with_db","text":"","code":"validate_schema_with_db(db_conn, schema_json_list, table_name = \"records\")"},{"path":"https://n8layman.github.io/ecoextract/reference/validate_schema_with_db.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate schema compatibility with database (internal) — validate_schema_with_db","text":"db_conn Database connection schema_json_list Parsed JSON schema list table_name Database table name validate ","code":""},{"path":"https://n8layman.github.io/ecoextract/reference/validate_schema_with_db.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate schema compatibility with database (internal) — validate_schema_with_db","text":"List validation results","code":""}]
